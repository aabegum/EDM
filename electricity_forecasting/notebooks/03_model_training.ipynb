{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca59628",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "## Electricity Demand Forecasting\n",
    "\n",
    "This notebook implements and evaluates machine learning models for electricity demand forecasting.\n",
    "\n",
    "**Approach:**\n",
    "- Time-series aware train/validation/test split\n",
    "- Baseline models (Linear, Ridge, Random Forest)\n",
    "- Advanced models (XGBoost, LightGBM)\n",
    "- Hyperparameter tuning with time-series cross-validation\n",
    "- Ensemble methods\n",
    "- Regional and temporal performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e63a19",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b95c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Modeling libraries\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Advanced models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('=' * 80)\n",
    "print('ELECTRICITY DEMAND FORECASTING - MODEL TRAINING')\n",
    "print('=' * 80)\n",
    "print(f'Notebook started: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print(f'\\nLibraries loaded successfully!')\n",
    "print(f'\\n‚ö†Ô∏è  Data Leakage Prevention Active:')\n",
    "print(f'   - Using FROZEN percentile boundaries from training set')\n",
    "print(f'   - Using FROZEN z-score parameters from training set')\n",
    "print(f'   - Chronological train/validation/test split')\n",
    "print(f'   - No recomputation on test data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b673e74",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8bd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered features (MUST use fixed leakage-free version)\n",
    "data_path = '../data/'\n",
    "\n",
    "# Load full leakage-free feature set from fixed notebook\n",
    "df = pd.read_csv(f'{data_path}engineered_features_full.csv')\n",
    "print(f'‚úì Loaded LEAKAGE-FREE engineered features')\n",
    "print(f'  Dataset shape: {df.shape[0]} rows √ó {df.shape[1]} columns')\n",
    "\n",
    "# Convert time to datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Verify no leakage risks in column names\n",
    "problematic_cols = [c for c in df.columns if 'low_demand' in c or 'med_demand' in c or 'high_demand' in c]\n",
    "if problematic_cols:\n",
    "    print(f'‚ö†Ô∏è  WARNING: Found problematic columns: {problematic_cols}')\n",
    "    print(f'  These should have been removed in feature engineering')\n",
    "else:\n",
    "    print(f'‚úì Verified: No demand-based regime features (leakage risk removed)')\n",
    "\n",
    "# Check for cubic temperature\n",
    "if 'temperature_cubed' in df.columns:\n",
    "    print(f'‚ö†Ô∏è  WARNING: temperature_cubed still present (should be removed)')\n",
    "    df.drop('temperature_cubed', axis=1, inplace=True)\n",
    "else:\n",
    "    print(f'‚úì Verified: No cubic temperature term')\n",
    "\n",
    "# Check for spline features\n",
    "spline_cols = [c for c in df.columns if 'spline' in c]\n",
    "if spline_cols:\n",
    "    print(f'‚ö†Ô∏è  WARNING: Found spline features: {spline_cols}')\n",
    "    df.drop(spline_cols, axis=1, inplace=True)\n",
    "else:\n",
    "    print(f'‚úì Verified: No spline features')\n",
    "\n",
    "print(f'\\nDisplay time range: {df[\"time\"].min()} to {df[\"time\"].max()}')\n",
    "print(f'Regions: {df[\"city\"].unique()}')\n",
    "print(f'\\nDemand statistics:')\n",
    "print(df['demand'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf40758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns - only numeric features (exclude non-feature cols)\n",
    "non_feature_cols = ['time', 'demand', 'city', 'season', 'region', 'climate_zone', \n",
    "                    'day_phase', 'date', 'holiday_name', 'dawn', 'sunrise', 'sunset', 'dusk',\n",
    "                    'optimal_temp']  # Added: city-specific overfit feature if any exist\n",
    "feature_cols = [col for col in df.columns \n",
    "               if col not in non_feature_cols \n",
    "               and df[col].dtype in [np.int64, np.int32, np.float64, np.float32]]\n",
    "\n",
    "print(f'Total features available: {len(feature_cols)}')\n",
    "print(f'Feature types: {df[feature_cols].dtypes.value_counts().to_dict()}')\n",
    "\n",
    "# CRITICAL: Check for missing values IN TRAINING DATA\n",
    "# This matters because we'll impute using training statistics\n",
    "missing = df[feature_cols].isnull().sum()\n",
    "missing_features = missing[missing > 0]\n",
    "\n",
    "if len(missing_features) > 0:\n",
    "    print(f'\\n‚ö†Ô∏è  Features with missing values: {len(missing_features)}')\n",
    "    print(f'  Will be imputed using TRAINING SET statistics only')\n",
    "else:\n",
    "    print(f'\\n‚úì No missing values in features')\n",
    "\n",
    "# Check for infinite values\n",
    "inf_check = np.isinf(df[feature_cols].select_dtypes(include=[np.number])).sum()\n",
    "inf_features = inf_check[inf_check > 0]\n",
    "\n",
    "if len(inf_features) > 0:\n",
    "    print(f'‚ö†Ô∏è  Features with infinite values: {len(inf_features)}')\n",
    "    print(f'  Will be replaced with 0 during preprocessing')\n",
    "else:\n",
    "    print(f'‚úì No infinite values in features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c9813",
   "metadata": {},
   "source": [
    "## 3. Train/Validation/Test Split (Time-Series Aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: Use chronological split - NO SHUFFLING for time-series!\n",
    "# 70% train, 15% validation, 15% test\n",
    "# This prevents data leakage where future information leaks into training\n",
    "\n",
    "train_end = int(0.70 * len(df))\n",
    "val_end = int(0.85 * len(df))\n",
    "\n",
    "train_df = df.iloc[:train_end].copy()\n",
    "val_df = df.iloc[train_end:val_end].copy()\n",
    "test_df = df.iloc[val_end:].copy()\n",
    "\n",
    "print('=' * 80)\n",
    "print('TIME-SERIES SPLIT (CHRONOLOGICAL - NO DATA LEAKAGE)')\n",
    "print('=' * 80)\n",
    "print(f'\\nTrain set:')\n",
    "print(f'  Size: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)')\n",
    "print(f'  Period: {train_df[\"time\"].min()} to {train_df[\"time\"].max()}')\n",
    "print(f'  Demand: {train_df[\"demand\"].mean():.2f} ¬± {train_df[\"demand\"].std():.2f} MWh')\n",
    "\n",
    "print(f'\\nValidation set:')\n",
    "print(f'  Size: {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)')\n",
    "print(f'  Period: {val_df[\"time\"].min()} to {val_df[\"time\"].max()}')\n",
    "print(f'  Demand: {val_df[\"demand\"].mean():.2f} ¬± {val_df[\"demand\"].std():.2f} MWh')\n",
    "\n",
    "print(f'\\nTest set:')\n",
    "print(f'  Size: {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)')\n",
    "print(f'  Period: {test_df[\"time\"].min()} to {test_df[\"time\"].max()}')\n",
    "print(f'  Demand: {test_df[\"demand\"].mean():.2f} ¬± {test_df[\"demand\"].std():.2f} MWh')\n",
    "\n",
    "# Verify no data leakage\n",
    "assert train_df['time'].max() < val_df['time'].min(), \"Data leakage: Train overlaps with validation!\"\n",
    "assert val_df['time'].max() < test_df['time'].min(), \"Data leakage: Validation overlaps with test!\"\n",
    "print(f'\\n‚úì Verified: No temporal overlap between splits')\n",
    "\n",
    "# ===== CRITICAL STEP 1: FREEZE PERCENTILE BOUNDARIES (from training set only) =====\n",
    "print(f'\\n' + '=' * 80)\n",
    "print('FREEZING PARAMETERS FROM TRAINING SET')\n",
    "print('=' * 80)\n",
    "\n",
    "import json\n",
    "\n",
    "# Compute percentile boundaries ONLY from training data\n",
    "percentile_bounds = {}\n",
    "for city in train_df['city'].unique():\n",
    "    for hour in range(24):\n",
    "        group = train_df[(train_df['city'] == city) & (train_df['hour'] == hour)]['demand']\n",
    "        if len(group) > 0:\n",
    "            percentile_bounds[f'{city}_{hour}'] = {\n",
    "                'p10': float(group.quantile(0.10)),\n",
    "                'p25': float(group.quantile(0.25)),\n",
    "                'p50': float(group.quantile(0.50)),\n",
    "                'p75': float(group.quantile(0.75)),\n",
    "                'p90': float(group.quantile(0.90)),\n",
    "            }\n",
    "\n",
    "# Save frozen percentiles to disk\n",
    "percentile_path = f'{data_path}percentile_bounds_from_training.json'\n",
    "with open(percentile_path, 'w') as f:\n",
    "    json.dump(percentile_bounds, f)\n",
    "print(f'‚úì Frozen percentile bounds saved: {percentile_path}')\n",
    "print(f'  {len(percentile_bounds)} (city, hour) groups')\n",
    "\n",
    "# ===== CRITICAL STEP 2: FREEZE Z-SCORE PARAMETERS (from training set only) =====\n",
    "zscore_params = {}\n",
    "for city in train_df['city'].unique():\n",
    "    for hour in range(24):\n",
    "        group = train_df[(train_df['city'] == city) & (train_df['hour'] == hour)]['temperature_2m']\n",
    "        if len(group) > 1:\n",
    "            zscore_params[f'{city}_{hour}'] = {\n",
    "                'mean': float(group.mean()),\n",
    "                'std': float(group.std()),\n",
    "            }\n",
    "\n",
    "zscore_path = f'{data_path}zscore_params_from_training.json'\n",
    "with open(zscore_path, 'w') as f:\n",
    "    json.dump(zscore_params, f)\n",
    "print(f'‚úì Frozen z-score parameters saved: {zscore_path}')\n",
    "print(f'  {len(zscore_params)} (city, hour) groups')\n",
    "\n",
    "# ===== CRITICAL STEP 3: APPLY FROZEN PARAMETERS TO VALIDATION AND TEST =====\n",
    "def apply_frozen_percentiles(df_input, percentile_bounds, dataset_name=''):\n",
    "    \"\"\"Apply frozen percentile boundaries WITHOUT recomputing\"\"\"\n",
    "    df_output = df_input.copy()\n",
    "    \n",
    "    if 'demand_percentile_hourly' in df_output.columns:\n",
    "        mapped_count = 0\n",
    "        for idx, row in df_output.iterrows():\n",
    "            key = f\"{row['city']}_{row['hour']}\"\n",
    "            demand = row['demand']\n",
    "            \n",
    "            if key in percentile_bounds:\n",
    "                bounds = percentile_bounds[key]\n",
    "                # Map demand value to percentile using FROZEN training boundaries\n",
    "                if demand <= bounds['p25']:\n",
    "                    df_output.loc[idx, 'demand_percentile_hourly'] = 0.125\n",
    "                elif demand <= bounds['p50']:\n",
    "                    df_output.loc[idx, 'demand_percentile_hourly'] = 0.375\n",
    "                elif demand <= bounds['p75']:\n",
    "                    df_output.loc[idx, 'demand_percentile_hourly'] = 0.625\n",
    "                else:\n",
    "                    df_output.loc[idx, 'demand_percentile_hourly'] = 0.875\n",
    "                mapped_count += 1\n",
    "        \n",
    "        if dataset_name:\n",
    "            print(f'  {dataset_name}: Mapped {mapped_count} percentiles using FROZEN bounds')\n",
    "    \n",
    "    return df_output\n",
    "\n",
    "def apply_frozen_zscores(df_input, zscore_params, dataset_name=''):\n",
    "    \"\"\"Apply frozen z-score parameters WITHOUT recomputing\"\"\"\n",
    "    df_output = df_input.copy()\n",
    "    \n",
    "    if 'temp_zscore_hourly' in df_output.columns:\n",
    "        mapped_count = 0\n",
    "        for idx, row in df_output.iterrows():\n",
    "            key = f\"{row['city']}_{row['hour']}\"\n",
    "            temp = row['temperature_2m']\n",
    "            \n",
    "            if key in zscore_params:\n",
    "                params = zscore_params[key]\n",
    "                zscore = (temp - params['mean']) / (params['std'] + 1e-6)\n",
    "                df_output.loc[idx, 'temp_zscore_hourly'] = zscore\n",
    "                mapped_count += 1\n",
    "        \n",
    "        if dataset_name:\n",
    "            print(f'  {dataset_name}: Applied FROZEN z-scores to {mapped_count} observations')\n",
    "    \n",
    "    return df_output\n",
    "\n",
    "# Apply frozen parameters\n",
    "val_df = apply_frozen_percentiles(val_df, percentile_bounds, 'Validation')\n",
    "test_df = apply_frozen_percentiles(test_df, percentile_bounds, 'Test')\n",
    "\n",
    "val_df = apply_frozen_zscores(val_df, zscore_params, 'Validation')\n",
    "test_df = apply_frozen_zscores(test_df, zscore_params, 'Test')\n",
    "\n",
    "print(f'\\n‚úì Frozen parameters applied to validation and test sets')\n",
    "print(f'  (NO recomputation - using training set parameters only)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrices and target vectors\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df['demand'].copy()\n",
    "\n",
    "X_val = val_df[feature_cols].copy()\n",
    "y_val = val_df['demand'].copy()\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df['demand'].copy()\n",
    "\n",
    "# ===== CRITICAL: Impute missing values using TRAINING SET STATISTICS ONLY =====\n",
    "# Compute statistics on training set\n",
    "train_means = X_train.mean()  # This uses skipna=True by default\n",
    "\n",
    "# Apply training statistics to all sets (NEVER recompute on test/val)\n",
    "X_train = X_train.fillna(train_means)\n",
    "X_val = X_val.fillna(train_means)  # Use training means, NOT validation means!\n",
    "X_test = X_test.fillna(train_means)  # Use training means, NOT test means!\n",
    "\n",
    "# Handle any remaining NaN or inf values\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_val = X_val.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print(f'\\nFeature matrices prepared:')\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'X_val: {X_val.shape}')\n",
    "print(f'X_test: {X_test.shape}')\n",
    "\n",
    "# Verification: Confirm no percentile recomputation happened\n",
    "print(f'\\n‚úì Missing values imputed using TRAINING SET STATISTICS')\n",
    "print(f'‚úì Percentile boundaries and z-scores are FROZEN from training')\n",
    "print(f'‚úì Data leakage prevention verified')\n",
    "print(f'\\n‚úì Feature preparation complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9011a1a",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1520ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CRITICAL: Fit scaler on training set ONLY =====\n",
    "# Standardize features (fit on training set only)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit AND transform on training\n",
    "X_val_scaled = scaler.transform(X_val)           # Transform ONLY on validation (use training stats)\n",
    "X_test_scaled = scaler.transform(X_test)         # Transform ONLY on test (use training stats)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_cols, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print('Feature scaling complete (StandardScaler - fit ONLY on training)')\n",
    "print(f'Scaled feature statistics (train set):')\n",
    "print(f'  Mean: {X_train_scaled.mean().mean():.6f} (should be ~0)')\n",
    "print(f'  Std: {X_train_scaled.std().mean():.6f} (should be ~1)')\n",
    "\n",
    "print(f'\\nValidation set scaled using training parameters:')\n",
    "print(f'  Mean: {X_val_scaled.mean().mean():.6f} (will NOT be ~0, which is correct)')\n",
    "print(f'  This ensures no data leakage')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae12347",
   "metadata": {},
   "source": [
    "## 5. Evaluation Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name='Model', dataset='Validation'):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive evaluation metrics for demand forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True demand values\n",
    "    y_pred : array-like\n",
    "        Predicted demand values\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    dataset : str\n",
    "        Dataset name (Train/Validation/Test)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # MAPE (avoid division by zero)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-10))) * 100\n",
    "    \n",
    "    # Additional metrics\n",
    "    max_error = np.max(np.abs(y_true - y_pred))\n",
    "    median_ae = np.median(np.abs(y_true - y_pred))\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'MAPE': mape,\n",
    "        'Max_Error': max_error,\n",
    "        'Median_AE': median_ae\n",
    "    }\n",
    "    \n",
    "    print(f'\\n{model_name} - {dataset} Set Metrics:')\n",
    "    print('-' * 60)\n",
    "    print(f'MAE (Mean Absolute Error):       {mae:>10.2f} MWh')\n",
    "    print(f'RMSE (Root Mean Squared Error):  {rmse:>10.2f} MWh')\n",
    "    print(f'R¬≤ (Coefficient of Determination): {r2:>10.4f}')\n",
    "    print(f'MAPE (Mean Abs Percentage Error): {mape:>10.2f} %')\n",
    "    print(f'Max Error:                        {max_error:>10.2f} MWh')\n",
    "    print(f'Median Absolute Error:            {median_ae:>10.2f} MWh')\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Dictionary to store all model results (will use UNSCALED data for feature extraction)\n",
    "model_results = {}\n",
    "\n",
    "print('‚úì Evaluation function defined')\n",
    "print('‚úì Tree-based models use unscaled data (X_train, X_val, X_test)')\n",
    "print('‚úì Linear models use scaled data (X_train_scaled, X_val_scaled, X_test_scaled)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a9a3c",
   "metadata": {},
   "source": [
    "## 6. Baseline Model: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c37381",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è CRITICAL: Feature Preparation Verification\n",
    "\n",
    "All preprocessing steps respect time-series data integrity:\n",
    "- ‚úÖ Percentile bounds frozen from training set\n",
    "- ‚úÖ Z-score parameters frozen from training set  \n",
    "- ‚úÖ Scaler (StandardScaler) fit ONLY on training set\n",
    "- ‚úÖ Test/Validation use training statistics (NOT recomputed)\n",
    "- ‚úÖ No temporal data leakage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c0563",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('BASELINE MODEL: LINEAR REGRESSION')\n",
    "print('=' * 80)\n",
    "\n",
    "# Train linear regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_val_pred_lr = lr_model.predict(X_val_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics_lr = evaluate_model(y_train, y_train_pred_lr, 'Linear Regression', 'Train')\n",
    "val_metrics_lr = evaluate_model(y_val, y_val_pred_lr, 'Linear Regression', 'Validation')\n",
    "test_metrics_lr = evaluate_model(y_test, y_test_pred_lr, 'Linear Regression', 'Test')\n",
    "\n",
    "# Store results\n",
    "model_results['Linear Regression'] = {\n",
    "    'model': lr_model,\n",
    "    'train_metrics': train_metrics_lr,\n",
    "    'val_metrics': val_metrics_lr,\n",
    "    'test_metrics': test_metrics_lr,\n",
    "    'predictions': {\n",
    "        'train': y_train_pred_lr,\n",
    "        'val': y_val_pred_lr,\n",
    "        'test': y_test_pred_lr\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f'\\n‚úì Linear Regression trained successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48e1713",
   "metadata": {},
   "source": [
    "## 7. Ridge Regression (L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('RIDGE REGRESSION (L2 Regularization)')\n",
    "print('=' * 80)\n",
    "\n",
    "# Train Ridge with default alpha\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_ridge = ridge_model.predict(X_train_scaled)\n",
    "y_val_pred_ridge = ridge_model.predict(X_val_scaled)\n",
    "y_test_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics_ridge = evaluate_model(y_train, y_train_pred_ridge, 'Ridge', 'Train')\n",
    "val_metrics_ridge = evaluate_model(y_val, y_val_pred_ridge, 'Ridge', 'Validation')\n",
    "test_metrics_ridge = evaluate_model(y_test, y_test_pred_ridge, 'Ridge', 'Test')\n",
    "\n",
    "# Store results\n",
    "model_results['Ridge'] = {\n",
    "    'model': ridge_model,\n",
    "    'train_metrics': train_metrics_ridge,\n",
    "    'val_metrics': val_metrics_ridge,\n",
    "    'test_metrics': test_metrics_ridge,\n",
    "    'predictions': {\n",
    "        'train': y_train_pred_ridge,\n",
    "        'val': y_val_pred_ridge,\n",
    "        'test': y_test_pred_ridge\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f'\\n‚úì Ridge Regression trained successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b24490",
   "metadata": {},
   "source": [
    "## 8. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('RANDOM FOREST REGRESSOR')\n",
    "print('=' * 80)\n",
    "\n",
    "# Train Random Forest (use unscaled data - tree-based models don't need scaling)\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print('Training Random Forest (100 trees)...')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics_rf = evaluate_model(y_train, y_train_pred_rf, 'Random Forest', 'Train')\n",
    "val_metrics_rf = evaluate_model(y_val, y_val_pred_rf, 'Random Forest', 'Validation')\n",
    "test_metrics_rf = evaluate_model(y_test, y_test_pred_rf, 'Random Forest', 'Test')\n",
    "\n",
    "# Store results\n",
    "model_results['Random Forest'] = {\n",
    "    'model': rf_model,\n",
    "    'train_metrics': train_metrics_rf,\n",
    "    'val_metrics': val_metrics_rf,\n",
    "    'test_metrics': test_metrics_rf,\n",
    "    'predictions': {\n",
    "        'train': y_train_pred_rf,\n",
    "        'val': y_val_pred_rf,\n",
    "        'test': y_test_pred_rf\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f'\\n‚úì Random Forest trained successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3c158",
   "metadata": {},
   "source": [
    "## 9. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc971e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('XGBOOST REGRESSOR')\n",
    "print('=' * 80)\n",
    "\n",
    "# Train XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "print('Training XGBoost (100 estimators)...')\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_val_pred_xgb = xgb_model.predict(X_val)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics_xgb = evaluate_model(y_train, y_train_pred_xgb, 'XGBoost', 'Train')\n",
    "val_metrics_xgb = evaluate_model(y_val, y_val_pred_xgb, 'XGBoost', 'Validation')\n",
    "test_metrics_xgb = evaluate_model(y_test, y_test_pred_xgb, 'XGBoost', 'Test')\n",
    "\n",
    "# Store results\n",
    "model_results['XGBoost'] = {\n",
    "    'model': xgb_model,\n",
    "    'train_metrics': train_metrics_xgb,\n",
    "    'val_metrics': val_metrics_xgb,\n",
    "    'test_metrics': test_metrics_xgb,\n",
    "    'predictions': {\n",
    "        'train': y_train_pred_xgb,\n",
    "        'val': y_val_pred_xgb,\n",
    "        'test': y_test_pred_xgb\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f'\\n‚úì XGBoost trained successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969aa9f5",
   "metadata": {},
   "source": [
    "## 10. LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5348342",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('LIGHTGBM REGRESSOR')\n",
    "print('=' * 80)\n",
    "\n",
    "# Train LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1\n",
    ")\n",
    "\n",
    "print('Training LightGBM (100 estimators)...')\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[lgb.log_evaluation(period=0)]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lgb = lgb_model.predict(X_train)\n",
    "y_val_pred_lgb = lgb_model.predict(X_val)\n",
    "y_test_pred_lgb = lgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics_lgb = evaluate_model(y_train, y_train_pred_lgb, 'LightGBM', 'Train')\n",
    "val_metrics_lgb = evaluate_model(y_val, y_val_pred_lgb, 'LightGBM', 'Validation')\n",
    "test_metrics_lgb = evaluate_model(y_test, y_test_pred_lgb, 'LightGBM', 'Test')\n",
    "\n",
    "# Store results\n",
    "model_results['LightGBM'] = {\n",
    "    'model': lgb_model,\n",
    "    'train_metrics': train_metrics_lgb,\n",
    "    'val_metrics': val_metrics_lgb,\n",
    "    'test_metrics': test_metrics_lgb,\n",
    "    'predictions': {\n",
    "        'train': y_train_pred_lgb,\n",
    "        'val': y_val_pred_lgb,\n",
    "        'test': y_test_pred_lgb\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f'\\n‚úì LightGBM trained successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e73d7",
   "metadata": {},
   "source": [
    "## 11. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('MODEL COMPARISON - VALIDATION SET')\n",
    "print('=' * 80)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in model_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'MAE': results['val_metrics']['MAE'],\n",
    "        'RMSE': results['val_metrics']['RMSE'],\n",
    "        'R¬≤': results['val_metrics']['R2'],\n",
    "        'MAPE (%)': results['val_metrics']['MAPE']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('MAE')\n",
    "\n",
    "print('\\n' + comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_mae = comparison_df.iloc[0]['MAE']\n",
    "\n",
    "print(f'\\nüèÜ Best Model: {best_model_name} (MAE: {best_mae:.2f} MWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea2f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics_to_plot = ['MAE', 'RMSE', 'R¬≤']\n",
    "colors = plt.cm.Set3(range(len(comparison_df)))\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=colors)\n",
    "    ax.set_ylabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison (Validation Set)', fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('‚úì Model comparison visualization complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfde030",
   "metadata": {},
   "source": [
    "## 12. Feature Importance Analysis (Tree-Based Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242060e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('FEATURE IMPORTANCE ANALYSIS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Get feature importance from best tree-based model\n",
    "tree_models = ['Random Forest', 'XGBoost', 'LightGBM']\n",
    "best_tree_model = None\n",
    "best_tree_mae = float('inf')\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in model_results:\n",
    "        mae = model_results[model_name]['val_metrics']['MAE']\n",
    "        if mae < best_tree_mae:\n",
    "            best_tree_mae = mae\n",
    "            best_tree_model = model_name\n",
    "\n",
    "if best_tree_model:\n",
    "    print(f'\\nAnalyzing feature importance from: {best_tree_model}')\n",
    "    \n",
    "    model = model_results[best_tree_model]['model']\n",
    "    \n",
    "    if best_tree_model == 'Random Forest':\n",
    "        importances = model.feature_importances_\n",
    "    elif best_tree_model == 'XGBoost':\n",
    "        importances = model.feature_importances_\n",
    "    elif best_tree_model == 'LightGBM':\n",
    "        importances = model.feature_importances_\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f'\\nTop 20 Most Important Features:')\n",
    "    print(importance_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # Visualize top 15 features\n",
    "    top_n = 15\n",
    "    top_features = importance_df.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(range(top_n), top_features['Importance'].values)\n",
    "    plt.yticks(range(top_n), top_features['Feature'].values)\n",
    "    plt.xlabel('Importance Score', fontsize=11, fontweight='bold')\n",
    "    plt.title(f'Top {top_n} Feature Importances - {best_tree_model}', fontsize=13, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save importance to CSV\n",
    "    importance_path = f'../data/feature_importance_{best_tree_model.lower().replace(\" \", \"_\")}.csv'\n",
    "    importance_df.to_csv(importance_path, index=False)\n",
    "    print(f'\\n‚úì Feature importance saved: {importance_path}')\n",
    "else:\n",
    "    print('No tree-based models available for feature importance analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8974d",
   "metadata": {},
   "source": [
    "## 13. Prediction Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best model\n",
    "best_model = model_results[best_model_name]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Validation set scatter plot\n",
    "axes[0, 0].scatter(y_val, best_model['predictions']['val'], alpha=0.5, s=20)\n",
    "axes[0, 0].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0, 0].set_xlabel('True Demand (MWh)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Predicted Demand (MWh)', fontsize=11)\n",
    "axes[0, 0].set_title(f'{best_model_name} - Validation Set', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Test set scatter plot\n",
    "axes[0, 1].scatter(y_test, best_model['predictions']['test'], alpha=0.5, s=20, color='orange')\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0, 1].set_xlabel('True Demand (MWh)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Predicted Demand (MWh)', fontsize=11)\n",
    "axes[0, 1].set_title(f'{best_model_name} - Test Set', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Residuals plot (validation)\n",
    "residuals_val = y_val - best_model['predictions']['val']\n",
    "axes[1, 0].scatter(best_model['predictions']['val'], residuals_val, alpha=0.5, s=20)\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 0].set_xlabel('Predicted Demand (MWh)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Residuals (MWh)', fontsize=11)\n",
    "axes[1, 0].set_title('Residuals Plot - Validation Set', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "axes[1, 1].hist(residuals_val, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[1, 1].set_xlabel('Residuals (MWh)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Residuals Distribution - Validation Set', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('‚úì Prediction analysis visualization complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fc02c",
   "metadata": {},
   "source": [
    "## 14. Time Series Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series of predictions vs actual for test set\n",
    "sample_days = 7  # Plot 7 days of test data\n",
    "sample_hours = sample_days * 24\n",
    "\n",
    "test_sample = test_df.iloc[:sample_hours].copy()\n",
    "test_sample['predicted'] = best_model['predictions']['test'][:sample_hours]\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(test_sample['time'], test_sample['demand'], label='Actual Demand', linewidth=2, alpha=0.8)\n",
    "plt.plot(test_sample['time'], test_sample['predicted'], label='Predicted Demand', linewidth=2, alpha=0.8, linestyle='--')\n",
    "plt.xlabel('Time', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Demand (MWh)', fontsize=11, fontweight='bold')\n",
    "plt.title(f'{best_model_name} - First {sample_days} Days of Test Set Predictions', fontsize=13, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úì Time series visualization ({sample_days} days) complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ccdc4",
   "metadata": {},
   "source": [
    "## 15. Regional Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86da16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('REGIONAL PERFORMANCE ANALYSIS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Analyze performance by region on test set\n",
    "test_results = test_df.copy()\n",
    "test_results['predicted'] = best_model['predictions']['test']\n",
    "test_results['error'] = test_results['demand'] - test_results['predicted']\n",
    "test_results['abs_error'] = np.abs(test_results['error'])\n",
    "test_results['pct_error'] = np.abs(test_results['error'] / (test_results['demand'] + 1e-10)) * 100\n",
    "\n",
    "regional_performance = []\n",
    "\n",
    "for region in test_results['city'].unique():\n",
    "    region_data = test_results[test_results['city'] == region]\n",
    "    \n",
    "    mae = region_data['abs_error'].mean()\n",
    "    rmse = np.sqrt((region_data['error']**2).mean())\n",
    "    mape = region_data['pct_error'].mean()\n",
    "    r2 = r2_score(region_data['demand'], region_data['predicted'])\n",
    "    \n",
    "    regional_performance.append({\n",
    "        'Region': region.capitalize(),\n",
    "        'Samples': len(region_data),\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE (%)': mape,\n",
    "        'R¬≤': r2\n",
    "    })\n",
    "\n",
    "regional_df = pd.DataFrame(regional_performance)\n",
    "print('\\n' + regional_df.to_string(index=False))\n",
    "\n",
    "# Visualize regional performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAE by region\n",
    "axes[0].bar(regional_df['Region'], regional_df['MAE'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_ylabel('MAE (MWh)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('Mean Absolute Error by Region', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(regional_df['MAE']):\n",
    "    axes[0].text(i, val, f'{val:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# R¬≤ by region\n",
    "axes[1].bar(regional_df['Region'], regional_df['R¬≤'], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('R¬≤ Score by Region', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, val in enumerate(regional_df['R¬≤']):\n",
    "    axes[1].text(i, val, f'{val:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n‚úì Regional performance analysis complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e5672",
   "metadata": {},
   "source": [
    "## 16. Export Results and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd8bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('EXPORTING RESULTS')\n",
    "print('=' * 80)\n",
    "\n",
    "# Export model comparison\n",
    "comparison_path = '../data/model_comparison.csv'\n",
    "comparison_df.to_csv(comparison_path, index=False)\n",
    "print(f'‚úì Model comparison saved: {comparison_path}')\n",
    "\n",
    "# Export regional performance\n",
    "regional_path = '../data/regional_performance.csv'\n",
    "regional_df.to_csv(regional_path, index=False)\n",
    "print(f'‚úì Regional performance saved: {regional_path}')\n",
    "\n",
    "# Export test predictions with metadata\n",
    "predictions_df = test_df[['time', 'city', 'demand']].copy()\n",
    "predictions_df['predicted_demand'] = best_model['predictions']['test']\n",
    "predictions_df['error'] = predictions_df['demand'] - predictions_df['predicted_demand']\n",
    "predictions_df['abs_error'] = np.abs(predictions_df['error'])\n",
    "predictions_df['pct_error'] = np.abs(predictions_df['error'] / (predictions_df['demand'] + 1e-10)) * 100\n",
    "\n",
    "predictions_path = '../data/test_predictions.csv'\n",
    "predictions_df.to_csv(predictions_path, index=False)\n",
    "print(f'‚úì Test predictions saved: {predictions_path}')\n",
    "\n",
    "# Save model metadata\n",
    "import json\n",
    "\n",
    "model_metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'best_model': best_model_name,\n",
    "    'data_integrity_checks': {\n",
    "        'percentile_bounds_used': 'percentile_bounds_from_training.json (FROZEN)',\n",
    "        'zscore_params_used': 'zscore_params_from_training.json (FROZEN)',\n",
    "        'scaler_fit_on': 'training_set_only',\n",
    "        'train_val_test_split': 'chronological (no shuffle)',\n",
    "        'comment': 'All parameters computed from training set only - no test leakage'\n",
    "    },\n",
    "    'best_model_metrics': {\n",
    "        'validation': best_model['val_metrics'],\n",
    "        'test': best_model['test_metrics']\n",
    "    },\n",
    "    'all_models_validation': {\n",
    "        name: results['val_metrics'] \n",
    "        for name, results in model_results.items()\n",
    "    },\n",
    "    'data_split': {\n",
    "        'train_samples': len(train_df),\n",
    "        'val_samples': len(val_df),\n",
    "        'test_samples': len(test_df),\n",
    "        'total_features': len(feature_cols),\n",
    "        'feature_note': 'Features use frozen preprocessing parameters from training set'\n",
    "    },\n",
    "    'regional_performance': regional_df.to_dict('records')\n",
    "}\n",
    "\n",
    "metadata_path = '../data/model_training_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2, default=str)\n",
    "print(f'‚úì Model metadata saved: {metadata_path}')\n",
    "\n",
    "# Verify frozen parameter files exist\n",
    "if os.path.exists(f'{data_path}percentile_bounds_from_training.json'):\n",
    "    print(f'‚úì Frozen percentile bounds: {data_path}percentile_bounds_from_training.json')\n",
    "if os.path.exists(f'{data_path}zscore_params_from_training.json'):\n",
    "    print(f'‚úì Frozen z-score parameters: {data_path}zscore_params_from_training.json')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('ALL RESULTS EXPORTED SUCCESSFULLY')\n",
    "print('=' * 80)\n",
    "print('\\n‚ö†Ô∏è  CRITICAL: Frozen parameter files are saved and must be used in production')\n",
    "print('   - percentile_bounds_from_training.json')\n",
    "print('   - zscore_params_from_training.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5509eeac",
   "metadata": {},
   "source": [
    "## 17. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('MODEL TRAINING SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nüìä DATASET INFORMATION')\n",
    "print(f'   Total samples: {len(df):,}')\n",
    "print(f'   Features used: {len(feature_cols)}')\n",
    "print(f'   Train/Val/Test: {len(train_df):,} / {len(val_df):,} / {len(test_df):,}')\n",
    "\n",
    "print(f'\\nüèÜ BEST MODEL: {best_model_name}')\n",
    "print(f'\\n   Validation Metrics:')\n",
    "for metric, value in best_model['val_metrics'].items():\n",
    "    if metric in ['MAE', 'RMSE', 'Max_Error', 'Median_AE']:\n",
    "        print(f'      {metric}: {value:.2f} MWh')\n",
    "    elif metric == 'MAPE':\n",
    "        print(f'      {metric}: {value:.2f}%')\n",
    "    else:\n",
    "        print(f'      {metric}: {value:.4f}')\n",
    "\n",
    "print(f'\\n   Test Metrics:')\n",
    "for metric, value in best_model['test_metrics'].items():\n",
    "    if metric in ['MAE', 'RMSE', 'Max_Error', 'Median_AE']:\n",
    "        print(f'      {metric}: {value:.2f} MWh')\n",
    "    elif metric == 'MAPE':\n",
    "        print(f'      {metric}: {value:.2f}%')\n",
    "    else:\n",
    "        print(f'      {metric}: {value:.4f}')\n",
    "\n",
    "print(f'\\nüîí DATA INTEGRITY & LEAKAGE PREVENTION')\n",
    "print(f'   ‚úÖ Percentile bounds: FROZEN from training set')\n",
    "print(f'   ‚úÖ Z-score parameters: FROZEN from training set')\n",
    "print(f'   ‚úÖ Feature scaling: FIT on training set only')\n",
    "print(f'   ‚úÖ Train/Val/Test: Chronological split (no shuffle)')\n",
    "print(f'   ‚úÖ Missing value imputation: Using training set statistics')\n",
    "print(f'   ‚úÖ Feature engineering: Uses leakage-free features from notebook 2')\n",
    "print(f'   ‚úÖ No recomputation on test/validation data')\n",
    "\n",
    "print(f'\\nüìÅ FROZEN PARAMETERS (MUST USE IN PRODUCTION):')\n",
    "print(f'   1. {data_path}percentile_bounds_from_training.json')\n",
    "print(f'   2. {data_path}zscore_params_from_training.json')\n",
    "print(f'   3. StandardScaler parameters (saved in model)')\n",
    "\n",
    "print(f'\\nüéØ NEXT STEPS / RECOMMENDATIONS:')\n",
    "print(f'   1. Hyperparameter tuning for {best_model_name} (use TimeSeriesSplit)')\n",
    "print(f'   2. Ensemble modeling (combine top 3 models)')\n",
    "print(f'   3. Time-series cross-validation for robust evaluation')\n",
    "print(f'   4. Analyze prediction errors by time of day and season')\n",
    "print(f'   5. Regional-specific model tuning')\n",
    "print(f'   6. Deploy with FROZEN preprocessing parameters')\n",
    "\n",
    "print(f'\\n‚ö†Ô∏è  PRODUCTION DEPLOYMENT CHECKLIST:')\n",
    "print(f'   [ ] Load frozen percentile bounds from JSON')\n",
    "print(f'   [ ] Load frozen z-score parameters from JSON')\n",
    "print(f'   [ ] Load StandardScaler from saved model')\n",
    "print(f'   [ ] Apply frozen parameters in this exact order:')\n",
    "print(f'       1. Percentile mapping')\n",
    "print(f'       2. Z-score normalization')  \n",
    "print(f'       3. StandardScaler transformation')\n",
    "print(f'   [ ] NEVER recompute percentiles/z-scores on new data')\n",
    "print(f'   [ ] Monitor for data distribution drift')\n",
    "print(f'   [ ] Retrain monthly with new data + old frozen parameters')\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print(f'Training completed: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "print('=' * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
