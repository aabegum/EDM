{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9ca59628",
            "metadata": {},
            "source": [
                "# Model Training and Evaluation\n",
                "## Electricity Demand Forecasting\n",
                "\n",
                "This notebook implements and evaluates machine learning models for electricity demand forecasting using the leakage-free feature set.\n",
                "\n",
                "**Key Components:**\n",
                "- **Data Loading**: Loads the cleaned, leakage-free dataset (`engineered_features_essential.csv`).\n",
                "- **Strict Chronological Split**: 70% Train, 15% Validation, 15% Test. NO shuffling.\n",
                "- **Leakage Prevention**: Freezes all statistical parameters (percentiles, z-scores, scaling means/stds) on the TRAINING set and applies them to Validation/Test.\n",
                "- **Model Training**: Baseline (Linear, Ridge) and Advanced (Random Forest, XGBoost, LightGBM).\n",
                "- **Evaluation**: Comprehensive metrics (MAE, RMSE, MAPE) on validation and test sets.\n",
                "- **Export**: Saves models, predictions, and most importantly, the **frozen parameters** for production use."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48b95c2b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from datetime import datetime\n",
                "import warnings\n",
                "import os\n",
                "import json\n",
                "import joblib\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Modeling libraries\n",
                "from sklearn.linear_model import LinearRegression, Ridge\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
                "\n",
                "# Advanced models\n",
                "import xgboost as xgb\n",
                "import lightgbm as lgb\n",
                "\n",
                "# Visualization settings\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print('=' * 80)\n",
                "print('ELECTRICITY DEMAND FORECASTING - MODEL TRAINING')\n",
                "print('=' * 80)\n",
                "print(f'Notebook initialized at: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6b673e74",
            "metadata": {},
            "source": [
                "## 1. Data Loading and feature verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7f8bd69b",
            "metadata": {},
            "outputs": [],
            "source": [
                "data_path = '../data/'\n",
                "input_file = f'{data_path}engineered_features_essential.csv'\n",
                "\n",
                "if not os.path.exists(input_file):\n",
                "    raise FileNotFoundError(f\"Input file not found: {input_file}. Please ensure feature engineering has generated the essential dataset.\")\n",
                "\n",
                "df = pd.read_csv(input_file)\n",
                "df['time'] = pd.to_datetime(df['time'])\n",
                "\n",
                "print(f'‚úì Loaded dataset: {df.shape[0]} rows √ó {df.shape[1]} columns')\n",
                "print(f'  Time range: {df[\"time\"].min()} to {df[\"time\"].max()}')\n",
                "\n",
                "# Identify features (exclude non-numeric and metadata)\n",
                "metadata_cols = ['time', 'demand', 'city', 'season', 'region', 'climate_zone', \n",
                "                 'day_phase', 'date', 'holiday_name', 'dawn', 'sunrise', 'sunset', 'dusk']\n",
                "\n",
                "# Exclude any other object-type columns that might have slipped in\n",
                "feature_cols = [c for c in df.columns if c not in metadata_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
                "\n",
                "print(f'‚úì Identified {len(feature_cols)} numeric features for modeling')\n",
                "\n",
                "# FINAL LEAKAGE CHECK: Drop any lingering \"future\" columns if they exist\n",
                "drop_leakage = [c for c in df.columns if 'future' in c or 'target' in c and c != 'demand']\n",
                "if drop_leakage:\n",
                "    print(f'‚ö†Ô∏è Dropping potential leakage columns: {drop_leakage}')\n",
                "    df.drop(columns=drop_leakage, inplace=True)\n",
                "    feature_cols = [c for c in feature_cols if c not in drop_leakage]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d42c9813",
            "metadata": {},
            "source": [
                "## 2. Chronological Train/Validation/Test Split\n",
                "We use a strict time-based split to simulate real-world forecasting. **No shuffling** is allowed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c80f8241",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Ratios: 70% Train, 15% Val, 15% Test\n",
                "train_end = int(0.70 * len(df))\n",
                "val_end = int(0.85 * len(df))\n",
                "\n",
                "train_df = df.iloc[:train_end].copy()\n",
                "val_df = df.iloc[train_end:val_end].copy()\n",
                "test_df = df.iloc[val_end:].copy()\n",
                "\n",
                "print(f'Train set:      {len(train_df)} samples ({train_df[\"time\"].min()} to {train_df[\"time\"].max()})')\n",
                "print(f'Validation set: {len(val_df)} samples ({val_df[\"time\"].min()} to {val_df[\"time\"].max()})')\n",
                "print(f'Test set:       {len(test_df)} samples ({test_df[\"time\"].min()} to {test_df[\"time\"].max()})')\n",
                "\n",
                "# Verify no overlap\n",
                "assert train_df['time'].max() < val_df['time'].min(), \"Leakage detected: Train overlaps Validation\"\n",
                "assert val_df['time'].max() < test_df['time'].min(), \"Leakage detected: Validation overlaps Test\"\n",
                "print('‚úì Chronological split verified.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f4c37381",
            "metadata": {},
            "source": [
                "## 3. Handling Statistical Features (Leakage Prevention)\n",
                "For features like `demand_percentile` or `z-scores` that rely on distributions, we must calculation statistics on the **training set only** and freeze them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "07cb886c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# FEATURE FREEZING & IMPUTATION\n",
                "\n",
                "# 1. Imputation: Compute means on TRAIN only\n",
                "train_means = train_df[feature_cols].mean()\n",
                "train_df[feature_cols] = train_df[feature_cols].fillna(train_means)\n",
                "val_df[feature_cols] = val_df[feature_cols].fillna(train_means)\n",
                "test_df[feature_cols] = test_df[feature_cols].fillna(train_means)\n",
                "\n",
                "# 2. Frozen Percentiles (if applicable)\n",
                "# Construct bounds based on training data distribution per city/hour\n",
                "percentile_bounds = {}\n",
                "for city in train_df['city'].unique():\n",
                "    for hour in range(24):\n",
                "        mask = (train_df['city'] == city) & (train_df['hour'] == hour)\n",
                "        if mask.any():\n",
                "            subset = train_df.loc[mask, 'demand']\n",
                "            percentile_bounds[f'{city}_{hour}'] = {\n",
                "                'p25': float(subset.quantile(0.25)),\n",
                "                'p50': float(subset.quantile(0.50)),\n",
                "                'p75': float(subset.quantile(0.75))\n",
                "            }\n",
                "\n",
                "# Save frozen percentiles\n",
                "with open(f'{data_path}frozen_percentiles.json', 'w') as f:\n",
                "    json.dump(percentile_bounds, f)\n",
                "\n",
                "def apply_frozen_percentiles(df_in, bounds):\n",
                "    df_out = df_in.copy()\n",
                "    # Placeholder for logic mapping values to percentiles using frozen bounds\n",
                "    return df_out\n",
                "\n",
                "print('‚úì Imputation using training means applied.')\n",
                "print(f'‚úì Frozen percentile bounds saved to {data_path}frozen_percentiles.json')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d9011a1a",
            "metadata": {},
            "source": [
                "## 4. Feature Scaling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1520ce37",
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = StandardScaler()\n",
                "\n",
                "# Fit on TRAIN only\n",
                "X_train = train_df[feature_cols]\n",
                "y_train = train_df['demand']\n",
                "\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=train_df.index)\n",
                "\n",
                "# Transform Val/Test\n",
                "X_val = val_df[feature_cols]\n",
                "y_val = val_df['demand']\n",
                "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=feature_cols, index=val_df.index)\n",
                "\n",
                "X_test = test_df[feature_cols]\n",
                "y_test = test_df['demand']\n",
                "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=feature_cols, index=test_df.index)\n",
                "\n",
                "# Save scalar for production\n",
                "joblib.dump(scaler, f'{data_path}scaler.pkl')\n",
                "print('‚úì Scaler fitted on training data and saved.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cae12347",
            "metadata": {},
            "source": [
                "## 5. Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76cb9524",
            "metadata": {},
            "outputs": [],
            "source": [
                "results = {}\n",
                "\n",
                "def evaluate(y_true, y_pred, set_name):\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
                "    r2 = r2_score(y_true, y_pred)\n",
                "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape, 'R2': r2}\n",
                "\n",
                "# 1. Linear Regression (Baseline)\n",
                "print('Training Linear Regression...')\n",
                "lr = LinearRegression()\n",
                "lr.fit(X_train_scaled, y_train)\n",
                "lr_preds = lr.predict(X_val_scaled)\n",
                "results['LinearRegression'] = evaluate(y_val, lr_preds, 'Validation')\n",
                "\n",
                "# 2. Ridge Regression\n",
                "print('Training Ridge Regression...')\n",
                "ridge = Ridge(alpha=1.0)\n",
                "ridge.fit(X_train_scaled, y_train)\n",
                "ridge_preds = ridge.predict(X_val_scaled)\n",
                "results['Ridge'] = evaluate(y_val, ridge_preds, 'Validation')\n",
                "\n",
                "# 3. Random Forest\n",
                "print('Training Random Forest (this may take a while)...')\n",
                "rf = RandomForestRegressor(n_estimators=100, max_depth=15, n_jobs=-1, random_state=42)\n",
                "rf.fit(X_train, y_train) \n",
                "rf_preds = rf.predict(X_val)\n",
                "results['RandomForest'] = evaluate(y_val, rf_preds, 'Validation')\n",
                "\n",
                "# 4. XGBoost\n",
                "print('Training XGBoost...')\n",
                "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
                "xgb_model.fit(X_train, y_train)\n",
                "xgb_preds = xgb_model.predict(X_val)\n",
                "results['XGBoost'] = evaluate(y_val, xgb_preds, 'Validation')\n",
                "\n",
                "# 5. LightGBM\n",
                "print('Training LightGBM...')\n",
                "lgb_model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, n_jobs=-1, random_state=42, verbose=-1)\n",
                "lgb_model.fit(X_train, y_train)\n",
                "lgb_preds = lgb_model.predict(X_val)\n",
                "results['LightGBM'] = evaluate(y_val, lgb_preds, 'Validation')\n",
                "\n",
                "print('\\nModel Evaluation Results (Validation Set):')\n",
                "res_df = pd.DataFrame(results).T.sort_values('MAE')\n",
                "print(res_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a7e5672",
            "metadata": {},
            "source": [
                "## 6. Final Evaluation on Test Set\n",
                "We select the best performing model from validation and evaluate it on the hold-out Test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5ebd8bda",
            "metadata": {},
            "outputs": [],
            "source": [
                "best_model_name = res_df.index[0]\n",
                "print(f'\\nüèÜ Best Model: {best_model_name}')\n",
                "\n",
                "if best_model_name == 'RandomForest':\n",
                "    final_model = rf\n",
                "    test_preds = final_model.predict(X_test)\n",
                "elif best_model_name == 'XGBoost':\n",
                "    final_model = xgb_model\n",
                "    test_preds = final_model.predict(X_test)\n",
                "elif best_model_name == 'LightGBM':\n",
                "    final_model = lgb_model\n",
                "    test_preds = final_model.predict(X_test)\n",
                "elif best_model_name == 'Ridge':\n",
                "    final_model = ridge\n",
                "    test_preds = final_model.predict(X_test_scaled)\n",
                "else:\n",
                "    final_model = lr\n",
                "    test_preds = final_model.predict(X_test_scaled)\n",
                "\n",
                "test_metrics = evaluate(y_test, test_preds, 'Test')\n",
                "print(f'Test Set Performance ({best_model_name}):')\n",
                "print(test_metrics)\n",
                "\n",
                "# Export Results\n",
                "res_df.to_csv(f'{data_path}model_validation_results.csv')\n",
                "output_file = f'{data_path}test_predictions.csv'\n",
                "test_df_export = test_df.copy()\n",
                "test_df_export['predicted_demand'] = test_preds\n",
                "test_df_export[['time', 'city', 'demand', 'predicted_demand']].to_csv(output_file, index=False)\n",
                "\n",
                "print(f'‚úì Predictions saved to {output_file}')\n",
                "\n",
                "# Feature Importance (if tree-based)\n",
                "if hasattr(final_model, 'feature_importances_'):\n",
                "    importances = pd.DataFrame({\n",
                "        'feature': feature_cols,\n",
                "        'importance': final_model.feature_importances_\n",
                "    }).sort_values('importance', ascending=False)\n",
                "    importances.to_csv(f'{data_path}feature_importance.csv', index=False)\n",
                "    print(f'‚úì Feature importance saved to {data_path}feature_importance.csv')\n",
                "    print('\\nTop 10 Features:')\n",
                "    print(importances.head(10))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}