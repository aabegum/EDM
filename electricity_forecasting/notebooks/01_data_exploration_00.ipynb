{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "e42998a6",
            "metadata": {},
            "source": [
                "# Electricity Demand Forecasting: Data Exploration & Feature Analysis\n",
                "\n",
                "**Objective**: Load, clean, and analyze the electricity demand dataset to prepare for forecasting.\n",
                "\n",
                "**Workflow**:\n",
                "1.  **Configuration**: Setup paths and parameters.\n",
                "2.  **Data Loading**: Load raw data.\n",
                "3.  **Basic Inspection**: Check data types and missing values.\n",
                "4.  **EDA**: Visual analysis of demand patterns.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b3e2da0c",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "COMPREHENSIVE DATA EXPLORATION\n",
                "Electricity Demand Forecasting - ADM Regional Level\n",
                "Multi-City Weather Features\n",
                "\"\"\"\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "from scipy import stats\n",
                "from statsmodels.tsa.seasonal import seasonal_decompose\n",
                "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Setup paths\n",
                "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
                "INPUT_DIR = PROJECT_ROOT / 'data' / 'input'\n",
                "FIGURES_DIR = PROJECT_ROOT / 'data' / 'output' / 'figures' / 'exploration'\n",
                "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Configuration\n",
                "CITIES = ['aydin', 'denizli', 'mugla']\n",
                "\n",
                "# Set plotting style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette(\"husl\")\n",
                "plt.rcParams['figure.figsize'] = (15, 6)\n",
                "plt.rcParams['font.size'] = 11\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"ELECTRICITY DEMAND FORECASTING - COMPREHENSIVE DATA EXPLORATION\")\n",
                "print(\"ADM Regional Demand with Multi-City Weather Features\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f5f7c81d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 1: DATA LOADING AND STRUCTURE VERIFICATION\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 1: DATA LOADING AND STRUCTURE VERIFICATION\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Load all city files\n",
                "dfs = {}\n",
                "for city in CITIES:\n",
                "    filepath = INPUT_DIR / f'{city}.csv'\n",
                "    df = pd.read_csv(filepath)\n",
                "    df['time'] = pd.to_datetime(df['time'])\n",
                "    df = df.sort_values('time').reset_index(drop=True)\n",
                "    dfs[city] = df\n",
                "    \n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    print(f\"  Shape: {df.shape}\")\n",
                "    print(f\"  Columns: {df.shape[1]}\")\n",
                "    print(f\"  Date range: {df['time'].min()} to {df['time'].max()}\")\n",
                "    print(f\"  Duration: {(df['time'].max() - df['time'].min()).days / 365.25:.2f} years\")\n",
                "    print(f\"  Total hours: {len(df):,}\")\n",
                "\n",
                "# Verify demand data structure\n",
                "print(\"\\n\" + \"-\"*80)\n",
                "print(\"CRITICAL VERIFICATION: Demand Data Structure\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "demand_comparison = pd.DataFrame({\n",
                "    city: dfs[city]['demand'].values for city in CITIES\n",
                "})\n",
                "\n",
                "# Statistical comparison\n",
                "print(\"\\nDemand Statistics by City File:\")\n",
                "print(demand_comparison.describe().T)\n",
                "\n",
                "# Check if identical\n",
                "print(\"\\n✓ Checking if demand values are identical across cities:\")\n",
                "all_identical = True\n",
                "for i, city1 in enumerate(CITIES):\n",
                "    for city2 in CITIES[i+1:]:\n",
                "        max_diff = np.abs(dfs[city1]['demand'].values - dfs[city2]['demand'].values).max()\n",
                "        mean_diff = np.abs(dfs[city1]['demand'].values - dfs[city2]['demand'].values).mean()\n",
                "        \n",
                "        if max_diff < 1e-6:  # Effectively zero\n",
                "            print(f\"  {city1.upper()} vs {city2.upper()}: IDENTICAL (max diff: {max_diff:.2e})\")\n",
                "        else:\n",
                "            print(f\"  {city1.upper()} vs {city2.upper()}: DIFFERENT (max diff: {max_diff:.2f}, mean diff: {mean_diff:.2f})\")\n",
                "            all_identical = False\n",
                "\n",
                "if all_identical:\n",
                "    print(\"\\n\" + \"✓\"*40)\n",
                "    print(\"CONFIRMED: Demand is ADM regional-level (identical across all cities)\")\n",
                "    print(\"STRATEGY: Use multi-city weather features for regional demand modeling\")\n",
                "    print(\"✓\"*40)\n",
                "else:\n",
                "    print(\"\\n\" + \"⚠\"*40)\n",
                "    print(\"WARNING: Demand values are NOT identical - investigate data source\")\n",
                "    print(\"⚠\"*40)\n",
                "\n",
                "# Use first city as base for demand (since identical)\n",
                "base_df = dfs['aydin'].copy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a90f3c55",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 2: DATA QUALITY ASSESSMENT\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 2: DATA QUALITY ASSESSMENT\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 2.1 Missing Values\n",
                "print(\"\\n[2.1] Missing Values Analysis\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "missing_summary = []\n",
                "for city in CITIES:\n",
                "    missing = dfs[city].isnull().sum()\n",
                "    missing_pct = (missing / len(dfs[city])) * 100\n",
                "    missing_df = pd.DataFrame({\n",
                "        'Column': missing.index,\n",
                "        'Missing': missing.values,\n",
                "        'Percentage': missing_pct.values\n",
                "    })\n",
                "    missing_df = missing_df[missing_df['Missing'] > 0].sort_values('Missing', ascending=False)\n",
                "    \n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    if len(missing_df) > 0:\n",
                "        print(missing_df.head(10).to_string(index=False))\n",
                "    else:\n",
                "        print(\"  ✓ No missing values\")\n",
                "\n",
                "# 2.2 Time Continuity\n",
                "print(\"\\n[2.2] Time Series Continuity\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "for city in CITIES:\n",
                "    df = dfs[city]\n",
                "    time_diff = df['time'].diff()\n",
                "    expected_diff = pd.Timedelta(hours=1)\n",
                "    gaps = time_diff[time_diff != expected_diff].dropna()\n",
                "    \n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    print(f\"  Total hours: {len(df):,}\")\n",
                "    print(f\"  Expected hours: {int((df['time'].max() - df['time'].min()).total_seconds() / 3600) + 1:,}\")\n",
                "    print(f\"  Time gaps found: {len(gaps)}\")\n",
                "    \n",
                "    if len(gaps) > 0:\n",
                "        print(f\"  Gap details (first 5):\")\n",
                "        gap_indices = gaps.index[:5]\n",
                "        for idx in gap_indices:\n",
                "            print(f\"    {df.loc[idx-1, 'time']} → {df.loc[idx, 'time']} (gap: {time_diff.loc[idx]})\")\n",
                "\n",
                "# 2.3 Duplicate Timestamps\n",
                "print(\"\\n[2.3] Duplicate Timestamps\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "for city in CITIES:\n",
                "    duplicates = dfs[city]['time'].duplicated().sum()\n",
                "    print(f\"{city.upper()}: {duplicates} duplicates\")\n",
                "\n",
                "# 2.4 Data Types\n",
                "print(\"\\n[2.4] Data Types Overview\")\n",
                "print(\"-\"*80)\n",
                "print(base_df.dtypes.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7d9e4a1b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 3: DEMAND ANALYSIS (REGIONAL)\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 3: REGIONAL DEMAND ANALYSIS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "demand = base_df['demand'].values\n",
                "\n",
                "# 3.1 Basic Statistics\n",
                "print(\"\\n[3.1] Demand Statistics\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "demand_stats = base_df['demand'].describe()\n",
                "print(f\"Count:      {demand_stats['count']:,.0f} hours\")\n",
                "print(f\"Mean:       {demand_stats['mean']:,.2f} MW\")\n",
                "print(f\"Median:     {demand_stats['50%']:,.2f} MW\")\n",
                "print(f\"Std Dev:    {demand_stats['std']:,.2f} MW\")\n",
                "print(f\"Min:        {demand_stats['min']:,.2f} MW\")\n",
                "print(f\"Max:        {demand_stats['max']:,.2f} MW\")\n",
                "print(f\"Range:      {demand_stats['max'] - demand_stats['min']:,.2f} MW\")\n",
                "print(f\"CV:         {(demand_stats['std'] / demand_stats['mean']):.2%}\")\n",
                "print(f\"Skewness:   {stats.skew(demand):.3f}\")\n",
                "print(f\"Kurtosis:   {stats.kurtosis(demand):.3f}\")\n",
                "\n",
                "# 3.2 Percentiles\n",
                "print(\"\\n[3.2] Demand Percentiles\")\n",
                "print(\"-\"*80)\n",
                "percentiles = [1, 5, 10, 25, 50, 75, 90, 95, 99]\n",
                "for p in percentiles:\n",
                "    value = np.percentile(demand, p)\n",
                "    print(f\"  {p:2d}th percentile: {value:,.2f} MW\")\n",
                "\n",
                "# 3.3 Outlier Detection\n",
                "print(\"\\n[3.3] Outlier Detection\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "mean_demand = demand_stats['mean']\n",
                "std_demand = demand_stats['std']\n",
                "\n",
                "# 3-sigma rule\n",
                "outliers_3sigma_high = base_df[base_df['demand'] > mean_demand + 3*std_demand]\n",
                "outliers_3sigma_low = base_df[base_df['demand'] < mean_demand - 3*std_demand]\n",
                "\n",
                "print(f\"3-Sigma Method:\")\n",
                "print(f\"  High outliers (>μ+3σ): {len(outliers_3sigma_high)} ({len(outliers_3sigma_high)/len(base_df)*100:.3f}%)\")\n",
                "print(f\"  Low outliers (<μ-3σ):  {len(outliers_3sigma_low)} ({len(outliers_3sigma_low)/len(base_df)*100:.3f}%)\")\n",
                "\n",
                "# IQR method\n",
                "Q1 = demand_stats['25%']\n",
                "Q3 = demand_stats['75%']\n",
                "IQR = Q3 - Q1\n",
                "outliers_iqr_high = base_df[base_df['demand'] > Q3 + 1.5*IQR]\n",
                "outliers_iqr_low = base_df[base_df['demand'] < Q1 - 1.5*IQR]\n",
                "\n",
                "print(f\"\\nIQR Method (1.5×IQR):\")\n",
                "print(f\"  High outliers: {len(outliers_iqr_high)} ({len(outliers_iqr_high)/len(base_df)*100:.2f}%)\")\n",
                "print(f\"  Low outliers:  {len(outliers_iqr_low)} ({len(outliers_iqr_low)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# Show extreme cases\n",
                "print(f\"\\n[3.4] Extreme Demand Events\")\n",
                "print(\"-\"*80)\n",
                "print(\"\\nTop 10 Highest Demand Hours:\")\n",
                "top_demands = base_df.nlargest(10, 'demand')[['time', 'demand', 'hour', 'month', \n",
                "                                                'is_weekend', 'is_holiday']]\n",
                "top_demands_display = top_demands.copy()\n",
                "top_demands_display['demand'] = top_demands_display['demand'].round(2)\n",
                "print(top_demands_display.to_string(index=False))\n",
                "\n",
                "print(\"\\nTop 10 Lowest Demand Hours:\")\n",
                "low_demands = base_df.nsmallest(10, 'demand')[['time', 'demand', 'hour', 'month',\n",
                "                                                 'is_weekend', 'is_holiday']]\n",
                "low_demands_display = low_demands.copy()\n",
                "low_demands_display['demand'] = low_demands_display['demand'].round(2)\n",
                "print(low_demands_display.to_string(index=False))\n",
                "\n",
                "# Visualization: Demand Distribution\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "# Histogram\n",
                "axes[0, 0].hist(demand, bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
                "axes[0, 0].axvline(mean_demand, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_demand:.0f}')\n",
                "axes[0, 0].axvline(mean_demand + 3*std_demand, color='orange', linestyle='--', linewidth=1.5, label='±3σ')\n",
                "axes[0, 0].axvline(mean_demand - 3*std_demand, color='orange', linestyle='--', linewidth=1.5)\n",
                "axes[0, 0].set_xlabel('Demand (MW)', fontsize=12)\n",
                "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
                "axes[0, 0].set_title('Demand Distribution with Outlier Bounds', fontsize=14, fontweight='bold')\n",
                "axes[0, 0].legend(fontsize=10)\n",
                "axes[0, 0].grid(alpha=0.3)\n",
                "\n",
                "# Q-Q Plot\n",
                "stats.probplot(demand, dist=\"norm\", plot=axes[0, 1])\n",
                "axes[0, 1].set_title('Q-Q Plot (Normal Distribution)', fontsize=14, fontweight='bold')\n",
                "axes[0, 1].grid(alpha=0.3)\n",
                "\n",
                "# Box plot by year\n",
                "base_df['year'] = base_df['time'].dt.year\n",
                "years = sorted(base_df['year'].unique())\n",
                "year_data = [base_df[base_df['year'] == year]['demand'].values for year in years]\n",
                "bp = axes[1, 0].boxplot(year_data, labels=years, patch_artist=True)\n",
                "for patch in bp['boxes']:\n",
                "    patch.set_facecolor('lightblue')\n",
                "axes[1, 0].set_xlabel('Year', fontsize=12)\n",
                "axes[1, 0].set_ylabel('Demand (MW)', fontsize=12)\n",
                "axes[1, 0].set_title('Demand Distribution by Year', fontsize=14, fontweight='bold')\n",
                "axes[1, 0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Time series (full period - downsampled for visibility)\n",
                "# Show every 24th point (daily instead of hourly)\n",
                "sample_indices = np.arange(0, len(base_df), 24)\n",
                "axes[1, 1].plot(base_df['time'].iloc[sample_indices], \n",
                "                base_df['demand'].iloc[sample_indices], \n",
                "                linewidth=0.5, alpha=0.7, color='darkblue')\n",
                "axes[1, 1].set_xlabel('Time', fontsize=12)\n",
                "axes[1, 1].set_ylabel('Demand (MW)', fontsize=12)\n",
                "axes[1, 1].set_title('Demand Time Series (Daily Average)', fontsize=14, fontweight='bold')\n",
                "axes[1, 1].grid(alpha=0.3)\n",
                "plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / '01_demand_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Figure saved: {FIGURES_DIR / '01_demand_distribution.png'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6c5b2e8f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 4: TEMPORAL PATTERNS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 4: TEMPORAL PATTERNS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 4.1 Hourly Patterns\n",
                "print(\"\\n[4.1] Hourly Patterns\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "hourly_stats = base_df.groupby('hour')['demand'].agg([\n",
                "    ('mean', 'mean'),\n",
                "    ('std', 'std'),\n",
                "    ('min', 'min'),\n",
                "    ('max', 'max'),\n",
                "    ('median', 'median')\n",
                "])\n",
                "\n",
                "peak_hour = hourly_stats['mean'].idxmax()\n",
                "trough_hour = hourly_stats['mean'].idxmin()\n",
                "\n",
                "print(f\"Peak hour:        {peak_hour}:00 ({hourly_stats.loc[peak_hour, 'mean']:.0f} MW)\")\n",
                "print(f\"Trough hour:      {trough_hour}:00 ({hourly_stats.loc[trough_hour, 'mean']:.0f} MW)\")\n",
                "print(f\"Daily range:      {hourly_stats['mean'].max() - hourly_stats['mean'].min():.0f} MW\")\n",
                "print(f\"Peak/trough ratio: {hourly_stats['mean'].max() / hourly_stats['mean'].min():.2f}\")\n",
                "print(f\"Avg hourly std:   {hourly_stats['std'].mean():.2f} MW\")\n",
                "\n",
                "# 4.2 Daily Patterns\n",
                "print(\"\\n[4.2] Daily Patterns (Day of Week)\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "base_df['day_name'] = base_df['time'].dt.day_name()\n",
                "daily_stats = base_df.groupby('day_name')['demand'].agg(['mean', 'std']).reindex([\n",
                "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
                "])\n",
                "\n",
                "weekday_avg = base_df[base_df['is_weekend'] == 0]['demand'].mean()\n",
                "weekend_avg = base_df[base_df['is_weekend'] == 1]['demand'].mean()\n",
                "\n",
                "print(f\"Weekday average:  {weekday_avg:.0f} MW\")\n",
                "print(f\"Weekend average:  {weekend_avg:.0f} MW\")\n",
                "print(f\"Difference:       {weekday_avg - weekend_avg:.0f} MW ({(weekday_avg - weekend_avg)/weekend_avg*100:+.1f}%)\")\n",
                "print(f\"\\nDaily Statistics:\")\n",
                "print(daily_stats.round(2))\n",
                "\n",
                "# 4.3 Monthly Patterns\n",
                "print(\"\\n[4.3] Monthly Patterns\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "monthly_stats = base_df.groupby(base_df['time'].dt.month)['demand'].agg(['mean', 'std', 'min', 'max'])\n",
                "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
                "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
                "monthly_stats.index = month_names\n",
                "\n",
                "peak_month = monthly_stats['mean'].idxmax()\n",
                "trough_month = monthly_stats['mean'].idxmin()\n",
                "\n",
                "print(f\"Peak month:   {peak_month} ({monthly_stats.loc[peak_month, 'mean']:.0f} MW)\")\n",
                "print(f\"Trough month: {trough_month} ({monthly_stats.loc[trough_month, 'mean']:.0f} MW)\")\n",
                "print(f\"Seasonal range: {monthly_stats['mean'].max() - monthly_stats['mean'].min():.0f} MW\")\n",
                "print(f\"\\nMonthly Statistics:\")\n",
                "print(monthly_stats.round(2))\n",
                "\n",
                "# 4.4 Seasonal Patterns\n",
                "print(\"\\n[4.4] Seasonal Patterns\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "seasonal_stats = base_df.groupby('season')['demand'].agg(['mean', 'std', 'count'])\n",
                "print(seasonal_stats.round(2))\n",
                "\n",
                "# Visualization: Temporal Patterns\n",
                "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
                "\n",
                "# Row 1: Hourly patterns\n",
                "# 1.1 Average by hour\n",
                "axes[0, 0].plot(hourly_stats.index, hourly_stats['mean'], marker='o', \n",
                "                linewidth=2, markersize=6, color='darkblue')\n",
                "axes[0, 0].fill_between(hourly_stats.index, \n",
                "                         hourly_stats['mean'] - hourly_stats['std'],\n",
                "                         hourly_stats['mean'] + hourly_stats['std'],\n",
                "                         alpha=0.3, color='lightblue')\n",
                "axes[0, 0].set_xlabel('Hour of Day', fontsize=11)\n",
                "axes[0, 0].set_ylabel('Demand (MW)', fontsize=11)\n",
                "axes[0, 0].set_title('Average Demand by Hour (±1 SD)', fontsize=12, fontweight='bold')\n",
                "axes[0, 0].grid(alpha=0.3)\n",
                "axes[0, 0].set_xticks(range(0, 24, 2))\n",
                "\n",
                "# 1.2 Weekday vs Weekend by hour\n",
                "weekday_hourly = base_df[base_df['is_weekend'] == 0].groupby('hour')['demand'].mean()\n",
                "weekend_hourly = base_df[base_df['is_weekend'] == 1].groupby('hour')['demand'].mean()\n",
                "axes[0, 1].plot(weekday_hourly.index, weekday_hourly.values, \n",
                "                marker='o', label='Weekday', linewidth=2, markersize=5)\n",
                "axes[0, 1].plot(weekend_hourly.index, weekend_hourly.values, \n",
                "                marker='s', label='Weekend', linewidth=2, markersize=5)\n",
                "axes[0, 1].set_xlabel('Hour of Day', fontsize=11)\n",
                "axes[0, 1].set_ylabel('Demand (MW)', fontsize=11)\n",
                "axes[0, 1].set_title('Hourly Demand: Weekday vs Weekend', fontsize=12, fontweight='bold')\n",
                "axes[0, 1].legend(fontsize=10)\n",
                "axes[0, 1].grid(alpha=0.3)\n",
                "axes[0, 1].set_xticks(range(0, 24, 2))\n",
                "\n",
                "# 1.3 Hour variance\n",
                "axes[0, 2].bar(hourly_stats.index, hourly_stats['std'], \n",
                "               color='coral', edgecolor='black', alpha=0.7)\n",
                "axes[0, 2].set_xlabel('Hour of Day', fontsize=11)\n",
                "axes[0, 2].set_ylabel('Standard Deviation (MW)', fontsize=11)\n",
                "axes[0, 2].set_title('Demand Variability by Hour', fontsize=12, fontweight='bold')\n",
                "axes[0, 2].grid(axis='y', alpha=0.3)\n",
                "axes[0, 2].set_xticks(range(0, 24, 2))\n",
                "\n",
                "# Row 2: Daily and Monthly patterns\n",
                "# 2.1 Day of week\n",
                "axes[1, 0].bar(range(7), daily_stats['mean'].values, \n",
                "               color='steelblue', edgecolor='black', alpha=0.7)\n",
                "axes[1, 0].errorbar(range(7), daily_stats['mean'].values, \n",
                "                    yerr=daily_stats['std'].values, fmt='none', \n",
                "                    ecolor='black', capsize=5, capthick=2)\n",
                "axes[1, 0].set_xlabel('Day of Week', fontsize=11)\n",
                "axes[1, 0].set_ylabel('Demand (MW)', fontsize=11)\n",
                "axes[1, 0].set_title('Average Demand by Day of Week (±1 SD)', fontsize=12, fontweight='bold')\n",
                "axes[1, 0].set_xticks(range(7))\n",
                "axes[1, 0].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
                "axes[1, 0].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# 2.2 Monthly pattern\n",
                "axes[1, 1].bar(range(12), monthly_stats['mean'].values, \n",
                "               color='forestgreen', edgecolor='black', alpha=0.7)\n",
                "axes[1, 1].set_xlabel('Month', fontsize=11)\n",
                "axes[1, 1].set_ylabel('Demand (MW)', fontsize=11)\n",
                "axes[1, 1].set_title('Average Demand by Month', fontsize=12, fontweight='bold')\n",
                "axes[1, 1].set_xticks(range(12))\n",
                "axes[1, 1].set_xticklabels(month_names, rotation=45)\n",
                "axes[1, 1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# 2.3 Monthly pattern across years\n",
                "for year in sorted(base_df['year'].unique()):\n",
                "    year_data = base_df[base_df['year'] == year]\n",
                "    monthly_year = year_data.groupby(year_data['time'].dt.month)['demand'].mean()\n",
                "    axes[1, 2].plot(monthly_year.index, monthly_year.values, \n",
                "                    marker='o', label=str(year), linewidth=2, markersize=4, alpha=0.7)\n",
                "axes[1, 2].set_xlabel('Month', fontsize=11)\n",
                "axes[1, 2].set_ylabel('Demand (MW)', fontsize=11)\n",
                "axes[1, 2].set_title('Monthly Demand Pattern by Year', fontsize=12, fontweight='bold')\n",
                "axes[1, 2].legend(fontsize=9, ncol=2)\n",
                "axes[1, 2].grid(alpha=0.3)\n",
                "axes[1, 2].set_xticks(range(1, 13))\n",
                "\n",
                "# Row 3: Heatmaps and seasonal\n",
                "# 3.1 Heatmap: Hour vs Day of Week\n",
                "pivot_hour_dow = base_df.pivot_table(values='demand', index='hour', \n",
                "                                      columns='day_of_week', aggfunc='mean')\n",
                "im1 = axes[2, 0].imshow(pivot_hour_dow.values, aspect='auto', cmap='YlOrRd', origin='lower')\n",
                "axes[2, 0].set_xlabel('Day of Week', fontsize=11)\n",
                "axes[2, 0].set_ylabel('Hour of Day', fontsize=11)\n",
                "axes[2, 0].set_title('Demand Heatmap: Hour vs Day', fontsize=12, fontweight='bold')\n",
                "axes[2, 0].set_xticks(range(7))\n",
                "axes[2, 0].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
                "axes[2, 0].set_yticks(range(0, 24, 2))\n",
                "plt.colorbar(im1, ax=axes[2, 0], label='Demand (MW)')\n",
                "\n",
                "# 3.2 Heatmap: Hour vs Month\n",
                "pivot_hour_month = base_df.pivot_table(values='demand', index='hour', \n",
                "                                        columns=base_df['time'].dt.month, aggfunc='mean')\n",
                "im2 = axes[2, 1].imshow(pivot_hour_month.values, aspect='auto', cmap='RdYlGn_r', origin='lower')\n",
                "axes[2, 1].set_xlabel('Month', fontsize=11)\n",
                "axes[2, 1].set_ylabel('Hour of Day', fontsize=11)\n",
                "axes[2, 1].set_title('Demand Heatmap: Hour vs Month', fontsize=12, fontweight='bold')\n",
                "axes[2, 1].set_xticks(range(12))\n",
                "axes[2, 1].set_xticklabels(month_names, rotation=45)\n",
                "axes[2, 1].set_yticks(range(0, 24, 2))\n",
                "plt.colorbar(im2, ax=axes[2, 1], label='Demand (MW)')\n",
                "\n",
                "# 3.3 Seasonal box plot\n",
                "season_order = ['winter', 'spring', 'summer', 'fall']\n",
                "season_data = [base_df[base_df['season'] == s]['demand'].values \n",
                "               for s in season_order if s in base_df['season'].unique()]\n",
                "bp = axes[2, 2].boxplot(season_data, \n",
                "                         labels=[s.capitalize() for s in season_order if s in base_df['season'].unique()],\n",
                "                         patch_artist=True)\n",
                "colors = ['lightblue', 'lightgreen', 'coral', 'wheat']\n",
                "for patch, color in zip(bp['boxes'], colors):\n",
                "    patch.set_facecolor(color)\n",
                "axes[2, 2].set_ylabel('Demand (MW)', fontsize=11)\n",
                "axes[2, 2].set_title('Demand Distribution by Season', fontsize=12, fontweight='bold')\n",
                "axes[2, 2].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / '02_temporal_patterns.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Figure saved: {FIGURES_DIR / '02_temporal_patterns.png'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b450d47c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 5: MULTI-CITY WEATHER ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 5: MULTI-CITY WEATHER COMPARISON\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 5.1 Temperature Comparison\n",
                "print(\"\\n[5.1] Temperature Statistics by City\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "temp_comparison = pd.DataFrame({\n",
                "    city: dfs[city]['temperature'].describe() for city in CITIES\n",
                "}).T\n",
                "\n",
                "print(temp_comparison.round(2))\n",
                "\n",
                "# Temperature ranges\n",
                "print(\"\\n[5.2] Temperature Characteristics\")\n",
                "print(\"-\"*80)\n",
                "for city in CITIES:\n",
                "    temp = dfs[city]['temperature']\n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    print(f\"  Annual range:    {temp.max() - temp.min():.1f}°C\")\n",
                "    print(f\"  Typical range:   {temp.quantile(0.05):.1f}°C to {temp.quantile(0.95):.1f}°C (5th-95th percentile)\")\n",
                "    print(f\"  Summer avg (JJA): {dfs[city][dfs[city]['month'].isin([6,7,8])]['temperature'].mean():.1f}°C\")\n",
                "    print(f\"  Winter avg (DJF): {dfs[city][dfs[city]['month'].isin([12,1,2])]['temperature'].mean():.1f}°C\")\n",
                "\n",
                "# 5.3 Spatial Temperature Variation\n",
                "print(\"\\n[5.3] Spatial Temperature Variation\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Calculate hourly temperature differences between cities\n",
                "temp_diff_coastal_inland = dfs['mugla']['temperature'].values - dfs['denizli']['temperature'].values\n",
                "temp_diff_aydin_denizli = dfs['aydin']['temperature'].values - dfs['denizli']['temperature'].values\n",
                "\n",
                "print(f\"\\nTemperature Gradients:\")\n",
                "print(f\"  Muğla - Denizli (Coastal-Inland):\")\n",
                "print(f\"    Mean difference:   {temp_diff_coastal_inland.mean():.2f}°C\")\n",
                "print(f\"    Std difference:    {temp_diff_coastal_inland.std():.2f}°C\")\n",
                "print(f\"    Max difference:    {temp_diff_coastal_inland.max():.2f}°C\")\n",
                "print(f\"    Min difference:    {temp_diff_coastal_inland.min():.2f}°C\")\n",
                "\n",
                "print(f\"\\n  Aydın - Denizli:\")\n",
                "print(f\"    Mean difference:   {temp_diff_aydin_denizli.mean():.2f}°C\")\n",
                "print(f\"    Std difference:    {temp_diff_aydin_denizli.std():.2f}°C\")\n",
                "\n",
                "# Calculate correlation between city temperatures\n",
                "print(\"\\n[5.4] Temperature Correlation Between Cities\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "temp_corr = pd.DataFrame({\n",
                "    city: dfs[city]['temperature'].values for city in CITIES\n",
                "}).corr()\n",
                "\n",
                "print(temp_corr.round(3))\n",
                "\n",
                "# 5.4 Other Weather Variables\n",
                "print(\"\\n[5.5] Other Weather Variables - Summary Statistics\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "weather_vars = ['humidity', 'windspeed', 'precipitation', 'cloudcover']\n",
                "\n",
                "for var in weather_vars:\n",
                "    print(f\"\\n{var.upper()}:\")\n",
                "    var_comparison = pd.DataFrame({\n",
                "        city: dfs[city][var].describe()[['mean', 'std', 'min', 'max']] \n",
                "        for city in CITIES if var in dfs[city].columns\n",
                "    }).T\n",
                "    print(var_comparison.round(2))\n",
                "\n",
                "# Visualization: Multi-City Weather\n",
                "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
                "\n",
                "# Row 1: Temperature comparisons\n",
                "# 1.1 Temperature distributions\n",
                "for city in CITIES:\n",
                "    axes[0, 0].hist(dfs[city]['temperature'], bins=50, alpha=0.5, \n",
                "                    label=city.capitalize(), edgecolor='black')\n",
                "axes[0, 0].set_xlabel('Temperature (°C)', fontsize=11)\n",
                "axes[0, 0].set_ylabel('Frequency', fontsize=11)\n",
                "axes[0, 0].set_title('Temperature Distribution by City', fontsize=12, fontweight='bold')\n",
                "axes[0, 0].legend(fontsize=10)\n",
                "axes[0, 0].grid(alpha=0.3)\n",
                "\n",
                "# 1.2 Temperature box plots\n",
                "temp_data = [dfs[city]['temperature'].values for city in CITIES]\n",
                "bp = axes[0, 1].boxplot(temp_data, labels=[c.capitalize() for c in CITIES], \n",
                "                         patch_artist=True)\n",
                "for patch, color in zip(bp['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
                "    patch.set_facecolor(color)\n",
                "axes[0, 1].set_ylabel('Temperature (°C)', fontsize=11)\n",
                "axes[0, 1].set_title('Temperature Distribution Comparison', fontsize=12, fontweight='bold')\n",
                "axes[0, 1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# 1.3 Temperature correlation heatmap\n",
                "im1 = axes[0, 2].imshow(temp_corr.values, cmap='coolwarm', vmin=0.9, vmax=1.0)\n",
                "axes[0, 2].set_xticks(range(len(CITIES)))\n",
                "axes[0, 2].set_yticks(range(len(CITIES)))\n",
                "axes[0, 2].set_xticklabels([c.capitalize() for c in CITIES])\n",
                "axes[0, 2].set_yticklabels([c.capitalize() for c in CITIES])\n",
                "axes[0, 2].set_title('Temperature Correlation Matrix', fontsize=12, fontweight='bold')\n",
                "for i in range(len(CITIES)):\n",
                "    for j in range(len(CITIES)):\n",
                "        axes[0, 2].text(j, i, f'{temp_corr.values[i, j]:.3f}', \n",
                "                        ha='center', va='center', fontsize=10)\n",
                "plt.colorbar(im1, ax=axes[0, 2])\n",
                "\n",
                "# Row 2: Monthly temperature patterns by city\n",
                "for city in CITIES:\n",
                "    monthly_temp = dfs[city].groupby(dfs[city]['time'].dt.month)['temperature'].mean()\n",
                "    axes[1, 0].plot(monthly_temp.index, monthly_temp.values, \n",
                "                    marker='o', label=city.capitalize(), linewidth=2, markersize=5)\n",
                "axes[1, 0].set_xlabel('Month', fontsize=11)\n",
                "axes[1, 0].set_ylabel('Temperature (°C)', fontsize=11)\n",
                "axes[1, 0].set_title('Average Monthly Temperature by City', fontsize=12, fontweight='bold')\n",
                "axes[1, 0].legend(fontsize=10)\n",
                "axes[1, 0].grid(alpha=0.3)\n",
                "axes[1, 0].set_xticks(range(1, 13))\n",
                "axes[1, 0].set_xticklabels(month_names, rotation=45)\n",
                "\n",
                "# 2.2 Temperature gradient time series (sample)\n",
                "sample_hours = 7*24  # 1 week\n",
                "sample_df = base_df.head(sample_hours).copy()\n",
                "sample_df['temp_gradient'] = temp_diff_coastal_inland[:sample_hours]\n",
                "axes[1, 1].plot(sample_df['time'], sample_df['temp_gradient'], \n",
                "                linewidth=1, color='purple')\n",
                "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
                "axes[1, 1].set_xlabel('Time', fontsize=11)\n",
                "axes[1, 1].set_ylabel('Temperature Difference (°C)', fontsize=11)\n",
                "axes[1, 1].set_title('Coastal-Inland Temperature Gradient (Sample Week)', \n",
                "                     fontsize=12, fontweight='bold')\n",
                "axes[1, 1].grid(alpha=0.3)\n",
                "plt.setp(axes[1, 1].xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "# 2.3 Humidity comparison\n",
                "humidity_data = [dfs[city]['humidity'].values for city in CITIES]\n",
                "bp2 = axes[1, 2].boxplot(humidity_data, labels=[c.capitalize() for c in CITIES],\n",
                "                          patch_artist=True)\n",
                "for patch, color in zip(bp2['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
                "    patch.set_facecolor(color)\n",
                "axes[1, 2].set_ylabel('Humidity (%)', fontsize=11)\n",
                "axes[1, 2].set_title('Humidity Distribution by City', fontsize=12, fontweight='bold')\n",
                "axes[1, 2].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# Row 3: Other weather variables\n",
                "# 3.1 Precipitation comparison\n",
                "precip_monthly = pd.DataFrame({\n",
                "    city: dfs[city].groupby(dfs[city]['time'].dt.month)['precipitation'].sum()\n",
                "    for city in CITIES\n",
                "})\n",
                "for city in CITIES:\n",
                "    axes[2, 0].plot(precip_monthly.index, precip_monthly[city].values,\n",
                "                    marker='o', label=city.capitalize(), linewidth=2, markersize=5)\n",
                "axes[2, 0].set_xlabel('Month', fontsize=11)\n",
                "axes[2, 0].set_ylabel('Total Precipitation (mm)', fontsize=11)\n",
                "axes[2, 0].set_title('Total Monthly Precipitation by City', fontsize=12, fontweight='bold')\n",
                "axes[2, 0].legend(fontsize=10)\n",
                "axes[2, 0].grid(alpha=0.3)\n",
                "axes[2, 0].set_xticks(range(1, 13))\n",
                "axes[2, 0].set_xticklabels(month_names, rotation=45)\n",
                "\n",
                "# 3.2 Wind speed comparison\n",
                "wind_data = [dfs[city]['windspeed'].values for city in CITIES]\n",
                "bp3 = axes[2, 1].boxplot(wind_data, labels=[c.capitalize() for c in CITIES],\n",
                "                          patch_artist=True)\n",
                "for patch, color in zip(bp3['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
                "    patch.set_facecolor(color)\n",
                "axes[2, 1].set_ylabel('Wind Speed (m/s)', fontsize=11)\n",
                "axes[2, 1].set_title('Wind Speed Distribution by City', fontsize=12, fontweight='bold')\n",
                "axes[2, 1].grid(axis='y', alpha=0.3)\n",
                "\n",
                "# 3.3 Cloud cover comparison\n",
                "cloud_data = [dfs[city]['cloudcover'].values for city in CITIES]\n",
                "bp4 = axes[2, 2].boxplot(cloud_data, labels=[c.capitalize() for c in CITIES],\n",
                "                          patch_artist=True)\n",
                "for patch, color in zip(bp4['boxes'], ['lightblue', 'lightcoral', 'lightgreen']):\n",
                "    patch.set_facecolor(color)\n",
                "axes[2, 2].set_ylabel('Cloud Cover (%)', fontsize=11)\n",
                "axes[2, 2].set_title('Cloud Cover Distribution by City', fontsize=12, fontweight='bold')\n",
                "axes[2, 2].grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / '03_multi_city_weather.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Figure saved: {FIGURES_DIR / '03_multi_city_weather.png'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7e3eeaa",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 6: DEMAND-WEATHER RELATIONSHIPS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 6: DEMAND-WEATHER RELATIONSHIPS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 6.1 Correlation Analysis - Each City's Weather vs Regional Demand\n",
                "print(\"\\n[6.1] Correlation: City-Specific Weather vs Regional Demand\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "weather_vars_analysis = ['temperature', 'humidity', 'dewpoint', 'windspeed', \n",
                "                         'precipitation', 'cloudcover', 'pressure_msl']\n",
                "\n",
                "correlation_results = {}\n",
                "\n",
                "for city in CITIES:\n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    correlations = {}\n",
                "    for var in weather_vars_analysis:\n",
                "        if var in dfs[city].columns:\n",
                "            corr = np.corrcoef(dfs[city][var].values, dfs[city]['demand'].values)[0, 1]\n",
                "            correlations[var] = corr\n",
                "            print(f\"  {var:20s}: {corr:+.3f}\")\n",
                "    correlation_results[city] = correlations\n",
                "\n",
                "# 6.2 Temperature-Demand Relationship (Most Important)\n",
                "print(\"\\n[6.2] Temperature-Demand Relationship Analysis\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "for city in CITIES:\n",
                "    temp = dfs[city]['temperature'].values\n",
                "    demand = dfs[city]['demand'].values\n",
                "    \n",
                "    # Linear correlation\n",
                "    corr_linear = np.corrcoef(temp, demand)[0, 1]\n",
                "    \n",
                "    # Polynomial fit to check non-linearity\n",
                "    z = np.polyfit(temp, demand, 2)\n",
                "    p = np.poly1d(z)\n",
                "    r2_poly = 1 - (np.sum((demand - p(temp))**2) / np.sum((demand - demand.mean())**2))\n",
                "    \n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    print(f\"  Linear correlation:      {corr_linear:+.3f}\")\n",
                "    print(f\"  Polynomial R²:           {r2_poly:.3f}\")\n",
                "    \n",
                "    # Find optimal temperature (minimum demand)\n",
                "    temp_range = np.linspace(temp.min(), temp.max(), 1000)\n",
                "    optimal_temp = temp_range[np.argmin(p(temp_range))]\n",
                "    print(f\"  Optimal temperature:     {optimal_temp:.1f}°C (minimum demand)\")\n",
                "\n",
                "# 6.3 Temperature Bins Analysis\n",
                "print(\"\\n[6.3] Demand by Temperature Bins\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Create temperature bins\n",
                "temp_bins = np.arange(-5, 45, 5)\n",
                "bin_labels = [f\"{temp_bins[i]}-{temp_bins[i+1]}°C\" for i in range(len(temp_bins)-1)]\n",
                "\n",
                "for city in CITIES:\n",
                "    dfs[city]['temp_bin'] = pd.cut(dfs[city]['temperature'], bins=temp_bins, labels=bin_labels)\n",
                "    bin_stats = dfs[city].groupby('temp_bin')['demand'].agg(['mean', 'count'])\n",
                "    \n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    print(bin_stats.round(2))\n",
                "\n",
                "# 6.4 Heating vs Cooling Demand Proxies\n",
                "print(\"\\n[6.4] Heating and Cooling Demand Analysis\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Calculate heating and cooling degree hours for each city\n",
                "heating_base = 18  # Base temperature for heating\n",
                "cooling_base = 24  # Base temperature for cooling\n",
                "\n",
                "for city in CITIES:\n",
                "    temp = dfs[city]['temperature']\n",
                "    demand = dfs[city]['demand']\n",
                "    \n",
                "    # Calculate degree hours\n",
                "    heating_dh = np.maximum(heating_base - temp, 0)\n",
                "    cooling_dh = np.maximum(temp - cooling_base, 0)\n",
                "    \n",
                "    # Correlation with demand\n",
                "    corr_heating = np.corrcoef(heating_dh, demand)[0, 1]\n",
                "    corr_cooling = np.corrcoef(cooling_dh, demand)[0, 1]\n",
                "    \n",
                "    print(f\"\\n{city.upper()}:\")\n",
                "    print(f\"  Heating degree hours correlation:  {corr_heating:+.3f}\")\n",
                "    print(f\"  Cooling degree hours correlation:  {corr_cooling:+.3f}\")\n",
                "    \n",
                "    # Average demand in different temperature regimes\n",
                "    cold_mask = temp < heating_base\n",
                "    comfortable_mask = (temp >= heating_base) & (temp <= cooling_base)\n",
                "    hot_mask = temp > cooling_base\n",
                "    \n",
                "    print(f\"  Avg demand when cold (<{heating_base}°C):       {demand[cold_mask].mean():.0f} MW\")\n",
                "    print(f\"  Avg demand when comfortable ({heating_base}-{cooling_base}°C): {demand[comfortable_mask].mean():.0f} MW\")\n",
                "    print(f\"  Avg demand when hot (>{cooling_base}°C):        {demand[hot_mask].mean():.0f} MW\")\n",
                "\n",
                "# 6.5 Multi-City Weather Impact\n",
                "print(\"\\n[6.5] Spatial Weather Heterogeneity Impact\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Calculate regional temperature statistics\n",
                "regional_temp_mean = np.mean([dfs[city]['temperature'].values for city in CITIES], axis=0)\n",
                "regional_temp_std = np.std([dfs[city]['temperature'].values for city in CITIES], axis=0)\n",
                "regional_temp_range = (\n",
                "    np.max([dfs[city]['temperature'].values for city in CITIES], axis=0) -\n",
                "    np.min([dfs[city]['temperature'].values for city in CITIES], axis=0)\n",
                ")\n",
                "\n",
                "demand = base_df['demand'].values\n",
                "\n",
                "# Correlations\n",
                "corr_mean = np.corrcoef(regional_temp_mean, demand)[0, 1]\n",
                "corr_std = np.corrcoef(regional_temp_std, demand)[0, 1]\n",
                "corr_range = np.corrcoef(regional_temp_range, demand)[0, 1]\n",
                "\n",
                "print(f\"Regional Temperature Metrics vs Demand:\")\n",
                "print(f\"  Mean temperature:         {corr_mean:+.3f}\")\n",
                "print(f\"  Std dev (heterogeneity):  {corr_std:+.3f}\")\n",
                "print(f\"  Range (spatial variation): {corr_range:+.3f}\")\n",
                "\n",
                "print(f\"\\nSpatial Temperature Variation Statistics:\")\n",
                "print(f\"  Mean std across cities:    {regional_temp_std.mean():.2f}°C\")\n",
                "print(f\"  Max std across cities:     {regional_temp_std.max():.2f}°C\")\n",
                "print(f\"  Mean range across cities:  {regional_temp_range.mean():.2f}°C\")\n",
                "print(f\"  Max range across cities:   {regional_temp_range.max():.2f}°C\")\n",
                "\n",
                "# Visualization: Demand-Weather Relationships\n",
                "fig = plt.figure(figsize=(18, 12))\n",
                "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
                "\n",
                "# Row 1: Temperature-Demand scatter plots for each city\n",
                "for idx, city in enumerate(CITIES):\n",
                "    ax = fig.add_subplot(gs[0, idx])\n",
                "    \n",
                "    # Sample data for visibility (every 10th point)\n",
                "    sample_indices = np.arange(0, len(dfs[city]), 10)\n",
                "    temp_sample = dfs[city]['temperature'].iloc[sample_indices]\n",
                "    demand_sample = dfs[city]['demand'].iloc[sample_indices]\n",
                "    \n",
                "    # Scatter plot\n",
                "    ax.scatter(temp_sample, demand_sample, alpha=0.3, s=2, c='blue')\n",
                "    \n",
                "    # Polynomial fit\n",
                "    z = np.polyfit(dfs[city]['temperature'], dfs[city]['demand'], 2)\n",
                "    p = np.poly1d(z)\n",
                "    temp_range = np.linspace(dfs[city]['temperature'].min(), \n",
                "                             dfs[city]['temperature'].max(), 100)\n",
                "    ax.plot(temp_range, p(temp_range), 'r-', linewidth=2, label='Polynomial fit')\n",
                "    \n",
                "    ax.set_xlabel('Temperature (°C)', fontsize=11)\n",
                "    ax.set_ylabel('Demand (MW)', fontsize=11)\n",
                "    ax.set_title(f'{city.capitalize()} - Temp vs Demand', fontsize=12, fontweight='bold')\n",
                "    ax.grid(alpha=0.3)\n",
                "    ax.legend(fontsize=9)\n",
                "\n",
                "# Row 2: Demand by temperature bins\n",
                "ax1 = fig.add_subplot(gs[1, 0])\n",
                "for city in CITIES:\n",
                "    bin_means = dfs[city].groupby('temp_bin')['demand'].mean()\n",
                "    ax1.plot(range(len(bin_means)), bin_means.values, \n",
                "             marker='o', label=city.capitalize(), linewidth=2, markersize=6)\n",
                "ax1.set_xlabel('Temperature Bin', fontsize=11)\n",
                "ax1.set_ylabel('Average Demand (MW)', fontsize=11)\n",
                "ax1.set_title('Average Demand by Temperature Range', fontsize=12, fontweight='bold')\n",
                "ax1.set_xticks(range(len(bin_labels)))\n",
                "ax1.set_xticklabels(bin_labels, rotation=45, ha='right')\n",
                "ax1.legend(fontsize=10)\n",
                "ax1.grid(alpha=0.3)\n",
                "\n",
                "# Row 2: Heating vs Cooling demand\n",
                "ax2 = fig.add_subplot(gs[1, 1])\n",
                "city_data = dfs['aydin']  # Use one city as example\n",
                "temp = city_data['temperature']\n",
                "demand = city_data['demand']\n",
                "\n",
                "# Calculate degree hours\n",
                "heating_dh = np.maximum(18 - temp, 0)\n",
                "cooling_dh = np.maximum(temp - 24, 0)\n",
                "\n",
                "# Sample for visibility\n",
                "sample_indices = np.arange(0, len(city_data), 10)\n",
                "ax2.scatter(heating_dh.iloc[sample_indices], demand.iloc[sample_indices], \n",
                "           alpha=0.4, s=10, c='blue', label='Heating')\n",
                "ax2.scatter(cooling_dh.iloc[sample_indices], demand.iloc[sample_indices], \n",
                "           alpha=0.4, s=10, c='red', label='Cooling')\n",
                "ax2.set_xlabel('Degree Hours', fontsize=11)\n",
                "ax2.set_ylabel('Demand (MW)', fontsize=11)\n",
                "ax2.set_title('Heating/Cooling Degree Hours vs Demand', fontsize=12, fontweight='bold')\n",
                "ax2.legend(fontsize=10)\n",
                "ax2.grid(alpha=0.3)\n",
                "\n",
                "# Row 2: Correlation heatmap\n",
                "ax3 = fig.add_subplot(gs[1, 2])\n",
                "corr_matrix = []\n",
                "corr_labels = []\n",
                "for city in CITIES:\n",
                "    for var in ['temperature', 'humidity', 'windspeed']:\n",
                "        if var in dfs[city].columns:\n",
                "            corr = np.corrcoef(dfs[city][var].values, dfs[city]['demand'].values)[0, 1]\n",
                "            corr_matrix.append(corr)\n",
                "            corr_labels.append(f\"{city.capitalize()}\\n{var}\")\n",
                "\n",
                "corr_array = np.array(corr_matrix).reshape(len(CITIES), 3)\n",
                "im = ax3.imshow(corr_array, cmap='RdYlGn', vmin=-1, vmax=1, aspect='auto')\n",
                "ax3.set_xticks(range(3))\n",
                "ax3.set_xticklabels(['Temperature', 'Humidity', 'Wind Speed'])\n",
                "ax3.set_yticks(range(len(CITIES)))\n",
                "ax3.set_yticklabels([c.capitalize() for c in CITIES])\n",
                "ax3.set_title('Weather-Demand Correlations', fontsize=12, fontweight='bold')\n",
                "for i in range(len(CITIES)):\n",
                "    for j in range(3):\n",
                "        ax3.text(j, i, f'{corr_array[i, j]:+.2f}', \n",
                "                ha='center', va='center', fontsize=10)\n",
                "plt.colorbar(im, ax=ax3, label='Correlation')\n",
                "\n",
                "# Row 3: Regional temperature metrics\n",
                "ax4 = fig.add_subplot(gs[2, 0])\n",
                "sample_hours = 30*24  # 30 days\n",
                "sample_indices = slice(0, sample_hours)\n",
                "ax4.plot(base_df['time'].iloc[sample_indices], \n",
                "         regional_temp_mean[sample_indices], \n",
                "         label='Mean', linewidth=2)\n",
                "ax4.fill_between(base_df['time'].iloc[sample_indices],\n",
                "                 regional_temp_mean[sample_indices] - regional_temp_std[sample_indices],\n",
                "                 regional_temp_mean[sample_indices] + regional_temp_std[sample_indices],\n",
                "                 alpha=0.3, label='±1 SD (heterogeneity)')\n",
                "ax4.set_xlabel('Time', fontsize=11)\n",
                "ax4.set_ylabel('Temperature (°C)', fontsize=11)\n",
                "ax4.set_title('Regional Temperature with Spatial Variation (30 days)', \n",
                "             fontsize=12, fontweight='bold')\n",
                "ax4.legend(fontsize=10)\n",
                "ax4.grid(alpha=0.3)\n",
                "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "# Row 3: Spatial heterogeneity vs demand\n",
                "ax5 = fig.add_subplot(gs[2, 1])\n",
                "sample_indices = np.arange(0, len(base_df), 10)\n",
                "ax5.scatter(regional_temp_std[sample_indices], \n",
                "           demand[sample_indices],\n",
                "           alpha=0.3, s=10, c='purple')\n",
                "ax5.set_xlabel('Temperature Std Dev Across Cities (°C)', fontsize=11)\n",
                "ax5.set_ylabel('Demand (MW)', fontsize=11)\n",
                "ax5.set_title('Spatial Temperature Heterogeneity vs Demand', \n",
                "             fontsize=12, fontweight='bold')\n",
                "ax5.grid(alpha=0.3)\n",
                "\n",
                "# Row 3: Hour-Temperature interaction on demand\n",
                "ax6 = fig.add_subplot(gs[2, 2])\n",
                "# Create bins for temperature and hour\n",
                "temp_hour_pivot = base_df.copy()\n",
                "temp_hour_pivot['temp_category'] = pd.cut(temp_hour_pivot['temperature'], \n",
                "                                           bins=5, labels=['Very Cold', 'Cold', 'Moderate', 'Warm', 'Hot'])\n",
                "hourly_by_temp = temp_hour_pivot.groupby(['hour', 'temp_category'])['demand'].mean().unstack()\n",
                "\n",
                "for col in hourly_by_temp.columns:\n",
                "    ax6.plot(hourly_by_temp.index, hourly_by_temp[col], \n",
                "            marker='o', label=col, linewidth=2, markersize=4, alpha=0.7)\n",
                "ax6.set_xlabel('Hour of Day', fontsize=11)\n",
                "ax6.set_ylabel('Average Demand (MW)', fontsize=11)\n",
                "ax6.set_title('Hourly Demand Pattern by Temperature Category', \n",
                "             fontsize=12, fontweight='bold')\n",
                "ax6.legend(fontsize=9, ncol=2)\n",
                "ax6.grid(alpha=0.3)\n",
                "ax6.set_xticks(range(0, 24, 2))\n",
                "\n",
                "plt.savefig(FIGURES_DIR / '04_demand_weather_relationships.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Figure saved: {FIGURES_DIR / '04_demand_weather_relationships.png'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78e1ba02",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 7: AUTOCORRELATION AND STATIONARITY ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 7: AUTOCORRELATION AND STATIONARITY ANALYSIS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 7.1 Autocorrelation Analysis\n",
                "print(\"\\n[7.1] Autocorrelation Analysis\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "from statsmodels.tsa.stattools import acf, pacf\n",
                "\n",
                "demand_series = base_df['demand'].values\n",
                "\n",
                "# Calculate ACF and PACF\n",
                "acf_values = acf(demand_series, nlags=168)  # 1 week\n",
                "pacf_values = pacf(demand_series, nlags=168)\n",
                "\n",
                "# Find significant lags\n",
                "significant_lags = []\n",
                "confidence_interval = 1.96 / np.sqrt(len(demand_series))\n",
                "\n",
                "for lag in range(1, 169):\n",
                "    if abs(acf_values[lag]) > confidence_interval:\n",
                "        significant_lags.append(lag)\n",
                "\n",
                "print(f\"Number of significant lags (up to 168h): {len(significant_lags)}\")\n",
                "print(f\"\\nKey autocorrelation values:\")\n",
                "key_lags = [1, 24, 48, 72, 168, 336]\n",
                "for lag in key_lags:\n",
                "    if lag < len(acf_values):\n",
                "        print(f\"  Lag {lag:3d}h: {acf_values[lag]:.3f}\")\n",
                "\n",
                "# 7.2 Stationarity Tests\n",
                "print(\"\\n[7.2] Stationarity Tests\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "from statsmodels.tsa.stattools import adfuller, kpss\n",
                "\n",
                "# Augmented Dickey-Fuller test\n",
                "adf_result = adfuller(demand_series, autolag='AIC')\n",
                "print(f\"\\nAugmented Dickey-Fuller Test:\")\n",
                "print(f\"  ADF Statistic:   {adf_result[0]:.4f}\")\n",
                "print(f\"  p-value:         {adf_result[1]:.4f}\")\n",
                "print(f\"  Critical values:\")\n",
                "for key, value in adf_result[4].items():\n",
                "    print(f\"    {key}: {value:.4f}\")\n",
                "\n",
                "if adf_result[1] < 0.05:\n",
                "    print(f\"  → Reject null hypothesis: Series is STATIONARY\")\n",
                "else:\n",
                "    print(f\"  → Fail to reject null hypothesis: Series is NON-STATIONARY\")\n",
                "\n",
                "# KPSS test\n",
                "kpss_result = kpss(demand_series, regression='ct', nlags='auto')\n",
                "print(f\"\\nKPSS Test:\")\n",
                "print(f\"  KPSS Statistic:  {kpss_result[0]:.4f}\")\n",
                "print(f\"  p-value:         {kpss_result[1]:.4f}\")\n",
                "print(f\"  Critical values:\")\n",
                "for key, value in kpss_result[3].items():\n",
                "    print(f\"    {key}: {value:.4f}\")\n",
                "\n",
                "if kpss_result[1] > 0.05:\n",
                "    print(f\"  → Fail to reject null hypothesis: Series is STATIONARY\")\n",
                "else:\n",
                "    print(f\"  → Reject null hypothesis: Series is NON-STATIONARY\")\n",
                "\n",
                "# 7.3 Differencing Analysis\n",
                "print(\"\\n[7.3] Impact of Differencing\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# First difference\n",
                "demand_diff1 = np.diff(demand_series)\n",
                "adf_diff1 = adfuller(demand_diff1, autolag='AIC')\n",
                "\n",
                "# Seasonal difference (24h)\n",
                "demand_diff24 = demand_series[24:] - demand_series[:-24]\n",
                "adf_diff24 = adfuller(demand_diff24, autolag='AIC')\n",
                "\n",
                "print(f\"Original series:\")\n",
                "print(f\"  ADF statistic: {adf_result[0]:.4f}, p-value: {adf_result[1]:.4f}\")\n",
                "\n",
                "print(f\"\\nFirst difference:\")\n",
                "print(f\"  ADF statistic: {adf_diff1[0]:.4f}, p-value: {adf_diff1[1]:.4f}\")\n",
                "if adf_diff1[1] < 0.05:\n",
                "    print(f\"  → STATIONARY after first difference\")\n",
                "\n",
                "print(f\"\\nSeasonal difference (24h):\")\n",
                "print(f\"  ADF statistic: {adf_diff24[0]:.4f}, p-value: {adf_diff24[1]:.4f}\")\n",
                "if adf_diff24[1] < 0.05:\n",
                "    print(f\"  → STATIONARY after seasonal difference\")\n",
                "\n",
                "# 7.4 Decomposition\n",
                "print(\"\\n[7.4] Time Series Decomposition\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Use a subset for decomposition (computational efficiency)\n",
                "decomp_period = 24 * 365  # 1 year\n",
                "decomp_data = base_df.iloc[:decomp_period].copy()\n",
                "decomp_data = decomp_data.set_index('time')\n",
                "\n",
                "# Additive decomposition\n",
                "decomposition = seasonal_decompose(decomp_data['demand'], \n",
                "                                   model='additive', \n",
                "                                   period=24*7)  # Weekly seasonality\n",
                "\n",
                "trend_strength = 1 - (np.var(decomposition.resid.dropna()) / \n",
                "                      np.var(decomposition.trend.dropna() + decomposition.resid.dropna()))\n",
                "seasonal_strength = 1 - (np.var(decomposition.resid.dropna()) / \n",
                "                         np.var(decomposition.seasonal.dropna() + decomposition.resid.dropna()))\n",
                "\n",
                "print(f\"Decomposition Statistics (1 year sample):\")\n",
                "print(f\"  Trend strength:     {trend_strength:.3f}\")\n",
                "print(f\"  Seasonal strength:  {seasonal_strength:.3f}\")\n",
                "print(f\"  Residual std:       {decomposition.resid.std():.2f} MW\")\n",
                "\n",
                "# Visualization: Autocorrelation and Stationarity\n",
                "fig = plt.figure(figsize=(18, 12))\n",
                "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
                "\n",
                "# Row 1: ACF and PACF\n",
                "ax1 = fig.add_subplot(gs[0, :2])\n",
                "plot_acf(demand_series, lags=168, ax=ax1, alpha=0.05)\n",
                "ax1.set_xlabel('Lag (hours)', fontsize=11)\n",
                "ax1.set_ylabel('Autocorrelation', fontsize=11)\n",
                "ax1.set_title('Autocorrelation Function (ACF) - 168 hours', \n",
                "             fontsize=12, fontweight='bold')\n",
                "ax1.grid(alpha=0.3)\n",
                "\n",
                "ax2 = fig.add_subplot(gs[0, 2])\n",
                "plot_pacf(demand_series, lags=48, ax=ax2, alpha=0.05)\n",
                "ax2.set_xlabel('Lag (hours)', fontsize=11)\n",
                "ax2.set_ylabel('Partial Autocorrelation', fontsize=11)\n",
                "ax2.set_title('Partial ACF - 48 hours', fontsize=12, fontweight='bold')\n",
                "ax2.grid(alpha=0.3)\n",
                "\n",
                "# Row 2: Decomposition\n",
                "ax3 = fig.add_subplot(gs[1, 0])\n",
                "ax3.plot(decomposition.observed.index, decomposition.observed.values, linewidth=0.5)\n",
                "ax3.set_ylabel('Observed', fontsize=10)\n",
                "ax3.set_title('Original Series', fontsize=11, fontweight='bold')\n",
                "ax3.grid(alpha=0.3)\n",
                "plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "ax4 = fig.add_subplot(gs[1, 1])\n",
                "ax4.plot(decomposition.trend.index, decomposition.trend.values, linewidth=1, color='orange')\n",
                "ax4.set_ylabel('Trend', fontsize=10)\n",
                "ax4.set_title('Trend Component', fontsize=11, fontweight='bold')\n",
                "ax4.grid(alpha=0.3)\n",
                "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "ax5 = fig.add_subplot(gs[1, 2])\n",
                "# Show one week of seasonal component\n",
                "seasonal_week = decomposition.seasonal[:24*7]\n",
                "ax5.plot(range(len(seasonal_week)), seasonal_week.values, linewidth=2, color='green')\n",
                "ax5.set_xlabel('Hour of Week', fontsize=10)\n",
                "ax5.set_ylabel('Seasonal', fontsize=10)\n",
                "ax5.set_title('Seasonal Component (1 week)', fontsize=11, fontweight='bold')\n",
                "ax5.grid(alpha=0.3)\n",
                "\n",
                "# Row 3: Differencing analysis\n",
                "ax6 = fig.add_subplot(gs[2, 0])\n",
                "sample_hours = 7*24  # 1 week sample\n",
                "ax6.plot(range(sample_hours), demand_series[:sample_hours], linewidth=1)\n",
                "ax6.set_xlabel('Hour', fontsize=10)\n",
                "ax6.set_ylabel('Demand (MW)', fontsize=10)\n",
                "ax6.set_title('Original Series (1 week sample)', fontsize=11, fontweight='bold')\n",
                "ax6.grid(alpha=0.3)\n",
                "\n",
                "ax7 = fig.add_subplot(gs[2, 1])\n",
                "ax7.plot(range(sample_hours-1), demand_diff1[:sample_hours-1], linewidth=1, color='red')\n",
                "ax7.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
                "ax7.set_xlabel('Hour', fontsize=10)\n",
                "ax7.set_ylabel('Difference', fontsize=10)\n",
                "ax7.set_title('First Difference', fontsize=11, fontweight='bold')\n",
                "ax7.grid(alpha=0.3)\n",
                "\n",
                "ax8 = fig.add_subplot(gs[2, 2])\n",
                "ax8.plot(range(sample_hours-24), demand_diff24[:sample_hours-24], linewidth=1, color='purple')\n",
                "ax8.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
                "ax8.set_xlabel('Hour', fontsize=10)\n",
                "ax8.set_ylabel('Difference', fontsize=10)\n",
                "ax8.set_title('Seasonal Difference (24h)', fontsize=11, fontweight='bold')\n",
                "ax8.grid(alpha=0.3)\n",
                "\n",
                "plt.savefig(FIGURES_DIR / '05_autocorrelation_stationarity.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Figure saved: {FIGURES_DIR / '05_autocorrelation_stationarity.png'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "175673b7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 8: FEATURE CORRELATION ANALYSIS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 8: FEATURE CORRELATION ANALYSIS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 8.1 Prepare feature correlation matrix\n",
                "print(\"\\n[8.1] Computing Feature Correlations\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Select relevant features for correlation analysis\n",
                "feature_cols = ['demand', 'hour', 'day_of_week', 'month', 'is_weekend', \n",
                "                'is_holiday', 'temperature', 'humidity', 'dewpoint', \n",
                "                'windspeed', 'cloudcover', 'precipitation', 'pressure_msl']\n",
                "\n",
                "# Available features\n",
                "available_features = [col for col in feature_cols if col in base_df.columns]\n",
                "\n",
                "# Create correlation matrix\n",
                "corr_matrix = base_df[available_features].corr()\n",
                "\n",
                "# Extract demand correlations\n",
                "demand_corr = corr_matrix['demand'].sort_values(ascending=False)\n",
                "\n",
                "print(f\"\\nTop correlations with demand:\")\n",
                "print(demand_corr.head(10).to_string())\n",
                "\n",
                "print(f\"\\nBottom correlations with demand:\")\n",
                "print(demand_corr.tail(5).to_string())\n",
                "\n",
                "# 8.2 Multi-city weather correlations with demand\n",
                "print(\"\\n[8.2] Multi-City Weather Feature Correlations\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "multicity_corr = {}\n",
                "for city in CITIES:\n",
                "    city_weather_cols = ['temperature', 'humidity', 'dewpoint', 'windspeed']\n",
                "    city_corr = {}\n",
                "    for col in city_weather_cols:\n",
                "        if col in dfs[city].columns:\n",
                "            corr = np.corrcoef(dfs[city][col].values, dfs[city]['demand'].values)[0, 1]\n",
                "            city_corr[col] = corr\n",
                "    multicity_corr[city] = city_corr\n",
                "\n",
                "multicity_corr_df = pd.DataFrame(multicity_corr).T\n",
                "print(multicity_corr_df.round(3))\n",
                "\n",
                "# 8.3 Feature multicollinearity\n",
                "print(\"\\n[8.3] Multicollinearity Analysis (VIF)\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "\n",
                "# Select numerical features (exclude demand)\n",
                "numerical_features = [col for col in available_features \n",
                "                     if col != 'demand' and base_df[col].dtype in ['int64', 'float64']]\n",
                "\n",
                "# Calculate VIF\n",
                "vif_data = pd.DataFrame()\n",
                "vif_data[\"Feature\"] = numerical_features\n",
                "vif_data[\"VIF\"] = [variance_inflation_factor(base_df[numerical_features].values, i) \n",
                "                   for i in range(len(numerical_features))]\n",
                "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
                "\n",
                "print(vif_data.to_string(index=False))\n",
                "print(f\"\\nInterpretation:\")\n",
                "print(f\"  VIF < 5:   Low multicollinearity\")\n",
                "print(f\"  VIF 5-10:  Moderate multicollinearity\")\n",
                "print(f\"  VIF > 10:  High multicollinearity (consider removing)\")\n",
                "\n",
                "high_vif = vif_data[vif_data['VIF'] > 10]\n",
                "if len(high_vif) > 0:\n",
                "    print(f\"\\n⚠ Features with high VIF:\")\n",
                "    print(high_vif.to_string(index=False))\n",
                "\n",
                "# Visualization: Feature Correlations\n",
                "fig = plt.figure(figsize=(18, 10))\n",
                "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
                "\n",
                "# Full correlation matrix heatmap\n",
                "ax1 = fig.add_subplot(gs[0, :2])\n",
                "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
                "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
                "            cmap='RdYlGn', center=0, vmin=-1, vmax=1,\n",
                "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8},\n",
                "            ax=ax1, annot_kws={'size': 8})\n",
                "ax1.set_title('Feature Correlation Matrix', fontsize=12, fontweight='bold')\n",
                "\n",
                "# Demand correlations bar plot\n",
                "ax2 = fig.add_subplot(gs[0, 2])\n",
                "demand_corr_plot = demand_corr.drop('demand').head(10)\n",
                "colors = ['green' if x > 0 else 'red' for x in demand_corr_plot.values]\n",
                "ax2.barh(range(len(demand_corr_plot)), demand_corr_plot.values, color=colors, alpha=0.7)\n",
                "ax2.set_yticks(range(len(demand_corr_plot)))\n",
                "ax2.set_yticklabels(demand_corr_plot.index, fontsize=10)\n",
                "ax2.set_xlabel('Correlation with Demand', fontsize=11)\n",
                "ax2.set_title('Top 10 Demand Correlations', fontsize=12, fontweight='bold')\n",
                "ax2.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
                "ax2.grid(axis='x', alpha=0.3)\n",
                "\n",
                "# Multi-city weather correlation comparison\n",
                "ax3 = fig.add_subplot(gs[1, 0])\n",
                "x = np.arange(len(multicity_corr_df.columns))\n",
                "width = 0.25\n",
                "for i, city in enumerate(CITIES):\n",
                "    ax3.bar(x + i*width, multicity_corr_df.loc[city].values, \n",
                "           width, label=city.capitalize(), alpha=0.7)\n",
                "ax3.set_xlabel('Weather Variable', fontsize=11)\n",
                "ax3.set_ylabel('Correlation with Demand', fontsize=11)\n",
                "ax3.set_title('Weather Correlations by City', fontsize=12, fontweight='bold')\n",
                "ax3.set_xticks(x + width)\n",
                "ax3.set_xticklabels(multicity_corr_df.columns, rotation=45, ha='right')\n",
                "ax3.legend(fontsize=10)\n",
                "ax3.grid(axis='y', alpha=0.3)\n",
                "ax3.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
                "\n",
                "# VIF plot\n",
                "ax4 = fig.add_subplot(gs[1, 1])\n",
                "colors_vif = ['red' if x > 10 else 'orange' if x > 5 else 'green' \n",
                "              for x in vif_data['VIF'].values]\n",
                "ax4.barh(range(len(vif_data)), vif_data['VIF'].values, color=colors_vif, alpha=0.7)\n",
                "ax4.set_yticks(range(len(vif_data)))\n",
                "ax4.set_yticklabels(vif_data['Feature'].values, fontsize=9)\n",
                "ax4.set_xlabel('VIF', fontsize=11)\n",
                "ax4.set_title('Variance Inflation Factor', fontsize=12, fontweight='bold')\n",
                "ax4.axvline(x=5, color='orange', linestyle='--', linewidth=1, label='VIF=5')\n",
                "ax4.axvline(x=10, color='red', linestyle='--', linewidth=1, label='VIF=10')\n",
                "ax4.legend(fontsize=9)\n",
                "ax4.grid(axis='x', alpha=0.3)\n",
                "\n",
                "# Scatter matrix for key features\n",
                "ax5 = fig.add_subplot(gs[1, 2])\n",
                "key_features = ['demand', 'temperature', 'hour']\n",
                "scatter_df = base_df[key_features].sample(n=1000, random_state=42)  # Sample for visibility\n",
                "pd.plotting.scatter_matrix(scatter_df, ax=ax5, alpha=0.3, figsize=(6, 6), \n",
                "                          diagonal='hist', s=10)\n",
                "ax5.set_title('Scatter Matrix (Key Features)', fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.savefig(FIGURES_DIR / '06_feature_correlations.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Figure saved: {FIGURES_DIR / '06_feature_correlations.png'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "271bae8f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 9: SUMMARY AND RECOMMENDATIONS\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 9: DATA EXPLORATION SUMMARY AND RECOMMENDATIONS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"KEY FINDINGS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\n[1] DATA STRUCTURE:\")\n",
                "print(\"  ✓ Demand data is ADM regional-level (identical across all cities)\")\n",
                "print(\"  ✓ Weather data is city-specific (Denizli, Aydın, Muğla)\")\n",
                "print(f\"  → Total observations: {len(base_df):,} hours ({len(base_df)/(24*365):.2f} years)\")\n",
                "print(f\"  → Data quality: {(1 - base_df.isnull().sum().sum()/(base_df.shape[0]*base_df.shape[1]))*100:.1f}% complete\")\n",
                "\n",
                "print(\"\\n[2] DEMAND CHARACTERISTICS:\")\n",
                "print(f\"  → Mean demand: {base_df['demand'].mean():.0f} MW\")\n",
                "print(f\"  → Peak demand: {base_df['demand'].max():.0f} MW\")\n",
                "print(f\"  → Coefficient of variation: {(base_df['demand'].std()/base_df['demand'].mean()):.2%}\")\n",
                "print(f\"  → Peak hour: {hourly_stats['mean'].idxmax()}:00\")\n",
                "print(f\"  → Trough hour: {hourly_stats['mean'].idxmin()}:00\")\n",
                "print(f\"  → Weekend effect: {(weekday_avg - weekend_avg)/weekend_avg*100:+.1f}%\")\n",
                "\n",
                "print(\"\\n[3] TEMPORAL PATTERNS:\")\n",
                "print(f\"  → Strong hourly seasonality (ACF at lag 24h: {acf_values[24]:.3f})\")\n",
                "print(f\"  → Strong weekly seasonality (ACF at lag 168h: {acf_values[168]:.3f})\")\n",
                "print(f\"  → Trend strength: {trend_strength:.3f}\")\n",
                "print(f\"  → Seasonal strength: {seasonal_strength:.3f}\")\n",
                "print(f\"  → Series stationarity: {'Yes' if adf_result[1] < 0.05 else 'No (needs differencing)'}\")\n",
                "\n",
                "print(\"\\n[4] WEATHER-DEMAND RELATIONSHIPS:\")\n",
                "print(f\"  → Temperature correlation (Denizli): {correlation_results['denizli']['temperature']:+.3f}\")\n",
                "print(f\"  → Non-linear temperature effect: Strong (U-shaped curve)\")\n",
                "print(f\"  → Optimal temperature (min demand): ~{optimal_temp:.1f}°C\")\n",
                "print(f\"  → Heating demand dominant when T < 18°C\")\n",
                "print(f\"  → Cooling demand dominant when T > 24°C\")\n",
                "\n",
                "print(\"\\n[5] MULTI-CITY WEATHER INSIGHTS:\")\n",
                "print(f\"  → Temperature correlation between cities: {temp_corr.values[np.triu_indices_from(temp_corr.values, k=1)].mean():.3f}\")\n",
                "print(f\"  → Mean spatial temperature variation: {regional_temp_std.mean():.2f}°C\")\n",
                "print(f\"  → Coastal-inland gradient correlation: {corr_range:+.3f}\")\n",
                "print(f\"  → Spatial heterogeneity matters: {'Yes' if abs(corr_std) > 0.1 else 'Limited'}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"MODELING RECOMMENDATIONS\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\n[1] DATA STRATEGY:\")\n",
                "print(\"  ✓ Use REGIONAL model with multi-city weather features\")\n",
                "print(\"  ✗ Do NOT train separate models per city (demand is identical)\")\n",
                "print(\"  ✓ Include spatial weather features:\")\n",
                "print(\"     - Regional mean temperature\")\n",
                "print(\"     - Spatial temperature std/range\")\n",
                "print(\"     - Coastal-inland gradient\")\n",
                "\n",
                "print(\"\\n[2] ESSENTIAL FEATURES (Priority Order):\")\n",
                "print(\"  1. Demand lags: 24h, 168h, 336h (Very High)\")\n",
                "print(\"  2. Hour (cyclical encoding): sin/cos (Very High)\")\n",
                "print(\"  3. Multi-city temperature: All 3 cities (High)\")\n",
                "print(\"  4. Day of week (cyclical): sin/cos (High)\")\n",
                "print(\"  5. Rolling demand statistics: 24h, 168h (High)\")\n",
                "print(\"  6. Weekend/holiday flags (Medium-High)\")\n",
                "print(\"  7. Temperature squared (non-linear effects) (Medium)\")\n",
                "print(\"  8. Multi-city humidity (Medium)\")\n",
                "print(\"  9. Spatial temperature features (Medium)\")\n",
                "print(\"  10. Temperature-hour interactions (Medium)\")\n",
                "\n",
                "print(\"\\n[3] FEATURE ENGINEERING PRIORITIES:\")\n",
                "print(\"  ✓ Cyclical encoding for hour, day, month\")\n",
                "print(\"  ✓ Lag features: 1h, 24h, 48h, 168h, 336h\")\n",
                "print(\"  ✓ Rolling statistics: 24h and 168h windows\")\n",
                "print(\"  ✓ Temperature transformations: squared, degree hours\")\n",
                "print(\"  ✓ Spatial weather aggregations: mean, std, range across cities\")\n",
                "print(\"  ✓ Interaction features: temp×hour, temp×weekend\")\n",
                "\n",
                "print(\"\\n[4] MODEL SELECTION GUIDANCE:\")\n",
                "print(\"  → Gradient Boosting (LightGBM/XGBoost): RECOMMENDED\")\n",
                "print(\"     - Handles non-linear relationships well\")\n",
                "print(\"     - Good with multi-city weather features\")\n",
                "print(\"     - Expected MAPE: 2.0-3.0%\")\n",
                "print(\"  → Deep Learning (LSTM/GRU): COMPLEMENTARY\")\n",
                "print(\"     - Captures long-term dependencies\")\n",
                "print(\"     - Benefits from sequential patterns\")\n",
                "print(\"     - Expected MAPE: 2.5-3.5%\")\n",
                "print(\"  → Ensemble: BEST FOR SOTA\")\n",
                "print(\"     - Combine LightGBM + LSTM\")\n",
                "print(\"     - Expected MAPE: <2.5%\")\n",
                "\n",
                "print(\"\\n[5] VALIDATION STRATEGY:\")\n",
                "print(\"  ✓ Time-based split (NO random splitting)\")\n",
                "print(\"  ✓ Walk-forward validation\")\n",
                "print(\"  ✓ Evaluate on:\")\n",
                "print(\"     - Overall MAPE\")\n",
                "print(\"     - By hour of day\")\n",
                "print(\"     - Weekday vs weekend\")\n",
                "print(\"     - By season\")\n",
                "print(\"     - Peak demand hours specifically\")\n",
                "\n",
                "print(\"\\n[6] POTENTIAL CHALLENGES:\")\n",
                "print(\"  ⚠ High autocorrelation → Ensure proper lag features\")\n",
                "print(\"  ⚠ Non-stationary trend → Consider differencing or trend features\")\n",
                "print(\"  ⚠ Non-linear temperature → Use polynomial or splines\")\n",
                "print(\"  ⚠ Multicollinearity → Monitor VIF, use regularization\")\n",
                "print(\"  ⚠ Outliers during extreme weather → Robust scaling or clipping\")\n",
                "\n",
                "print(\"\\n[7] EXPECTED PERFORMANCE TARGETS:\")\n",
                "print(\"  → Baseline (simple features): MAPE 4-5%\")\n",
                "print(\"  → Good (core features): MAPE 3-4%\")\n",
                "print(\"  → Strong (full features): MAPE 2.5-3%\")\n",
                "print(\"  → SOTA (ensemble): MAPE <2.5%\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"NEXT STEPS\")\n",
                "print(\"=\"*80)\n",
                "print(\"  1. Implement regional data loading (load_regional_data)\")\n",
                "print(\"  2. Engineer all recommended features\")\n",
                "print(\"  3. Start with LightGBM baseline (~15-20 core features)\")\n",
                "print(\"  4. Expand to full feature set if baseline MAPE > 3%\")\n",
                "print(\"  5. Add LSTM model for ensemble\")\n",
                "print(\"  6. Optimize hyperparameters with Optuna\")\n",
                "print(\"  7. Create final ensemble model\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"DATA EXPLORATION COMPLETED\")\n",
                "print(f\"All figures saved to: {FIGURES_DIR}\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Save summary statistics\n",
                "summary_stats = {\n",
                "    'data_structure': {\n",
                "        'total_hours': int(len(base_df)),\n",
                "        'years': float(len(base_df)/(24*365)),\n",
                "        'cities': CITIES,\n",
                "        'demand_identical': all_identical\n",
                "    },\n",
                "    'demand_stats': {\n",
                "        'mean': float(base_df['demand'].mean()),\n",
                "        'std': float(base_df['demand'].std()),\n",
                "        'min': float(base_df['demand'].min()),\n",
                "        'max': float(base_df['demand'].max()),\n",
                "        'cv': float(base_df['demand'].std()/base_df['demand'].mean())\n",
                "    },\n",
                "    'temporal_patterns': {\n",
                "        'peak_hour': int(hourly_stats['mean'].idxmax()),\n",
                "        'trough_hour': int(hourly_stats['mean'].idxmin()),\n",
                "        'weekday_avg': float(weekday_avg),\n",
                "        'weekend_avg': float(weekend_avg),\n",
                "        'acf_24h': float(acf_values[24]),\n",
                "        'acf_168h': float(acf_values[168])\n",
                "    },\n",
                "    'weather_correlations': correlation_results,\n",
                "    'stationarity': {\n",
                "        'adf_statistic': float(adf_result[0]),\n",
                "        'adf_pvalue': float(adf_result[1]),\n",
                "        'is_stationary': bool(adf_result[1] < 0.05)\n",
                "    }\n",
                "}\n",
                "\n",
                "import json\n",
                "with open(FIGURES_DIR / 'exploration_summary.json', 'w') as f:\n",
                "    json.dump(summary_stats, f, indent=2)\n",
                "\n",
                "print(f\"\\n✓ Summary statistics saved: {FIGURES_DIR / 'exploration_summary.json'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0fcfb18c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SECTION 10: ADVANCED OUTLIER AND ANOMALY DETECTION\n",
                "# ============================================================================\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SECTION 10: ADVANCED OUTLIER AND ANOMALY DETECTION\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# 10.1 Statistical Outlier Detection Methods\n",
                "print(\"\\n[10.1] Statistical Outlier Detection\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "demand = base_df['demand'].values\n",
                "\n",
                "# Method 1: Z-Score\n",
                "z_scores = np.abs(stats.zscore(demand))\n",
                "zscore_outliers = base_df[z_scores > 3].copy()\n",
                "\n",
                "print(f\"\\nZ-Score Method (threshold=3):\")\n",
                "print(f\"  Outliers detected: {len(zscore_outliers)} ({len(zscore_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# Method 2: Modified Z-Score (more robust)\n",
                "median = np.median(demand)\n",
                "mad = np.median(np.abs(demand - median))\n",
                "modified_z_scores = 0.6745 * (demand - median) / mad\n",
                "modified_zscore_outliers = base_df[np.abs(modified_z_scores) > 3.5].copy()\n",
                "\n",
                "print(f\"\\nModified Z-Score Method (threshold=3.5):\")\n",
                "print(f\"  Outliers detected: {len(modified_zscore_outliers)} ({len(modified_zscore_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# Method 3: IQR Method\n",
                "Q1 = base_df['demand'].quantile(0.25)\n",
                "Q3 = base_df['demand'].quantile(0.75)\n",
                "IQR = Q3 - Q1\n",
                "iqr_outliers = base_df[(base_df['demand'] < Q1 - 1.5*IQR) | \n",
                "                        (base_df['demand'] > Q3 + 1.5*IQR)].copy()\n",
                "\n",
                "print(f\"\\nIQR Method (1.5×IQR):\")\n",
                "print(f\"  Lower bound: {Q1 - 1.5*IQR:.2f} MW\")\n",
                "print(f\"  Upper bound: {Q3 + 1.5*IQR:.2f} MW\")\n",
                "print(f\"  Outliers detected: {len(iqr_outliers)} ({len(iqr_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# Method 4: Isolation Forest\n",
                "from sklearn.ensemble import IsolationForest\n",
                "\n",
                "# Use demand and key features for context-aware detection\n",
                "isolation_features = ['demand', 'hour', 'day_of_week', 'month', 'temperature']\n",
                "isolation_data = base_df[isolation_features].copy().fillna(base_df[isolation_features].mean())\n",
                "\n",
                "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
                "isolation_predictions = iso_forest.fit_predict(isolation_data)\n",
                "isolation_outliers = base_df[isolation_predictions == -1].copy()\n",
                "\n",
                "print(f\"\\nIsolation Forest (contamination=0.01):\")\n",
                "print(f\"  Outliers detected: {len(isolation_outliers)} ({len(isolation_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# 10.2 Time Series Specific Anomaly Detection\n",
                "print(\"\\n[10.2] Time Series Anomaly Detection\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Method 1: Seasonal Decomposition Residuals\n",
                "print(f\"\\nSeasonal Decomposition Residuals:\")\n",
                "\n",
                "# Use decomposition from earlier (or recompute)\n",
                "decomp_period = min(24 * 365, len(base_df))  # 1 year or less\n",
                "decomp_data = base_df.iloc[:decomp_period].copy()\n",
                "decomp_data = decomp_data.set_index('time')\n",
                "\n",
                "decomposition = seasonal_decompose(decomp_data['demand'], \n",
                "                                   model='additive', \n",
                "                                   period=24*7,\n",
                "                                   extrapolate_trend='freq')\n",
                "\n",
                "# Outliers based on residuals\n",
                "residual_threshold = 3 * decomposition.resid.std()\n",
                "residual_outliers_idx = np.abs(decomposition.resid) > residual_threshold\n",
                "residual_outliers = decomp_data[residual_outliers_idx].copy()\n",
                "\n",
                "print(f\"  Residual std: {decomposition.resid.std():.2f} MW\")\n",
                "print(f\"  Threshold (3σ): {residual_threshold:.2f} MW\")\n",
                "print(f\"  Outliers detected: {residual_outliers_idx.sum()} ({residual_outliers_idx.sum()/len(decomp_data)*100:.2f}%)\")\n",
                "\n",
                "# Method 2: Moving Average Deviation\n",
                "window = 24  # 24-hour window\n",
                "base_df['rolling_mean'] = base_df['demand'].rolling(window=window, center=True).mean()\n",
                "base_df['rolling_std'] = base_df['demand'].rolling(window=window, center=True).std()\n",
                "base_df['deviation'] = np.abs(base_df['demand'] - base_df['rolling_mean'])\n",
                "base_df['z_deviation'] = base_df['deviation'] / base_df['rolling_std']\n",
                "\n",
                "moving_avg_outliers = base_df[base_df['z_deviation'] > 3].copy()\n",
                "\n",
                "print(f\"\\nMoving Average Deviation (24h window, threshold=3σ):\")\n",
                "print(f\"  Outliers detected: {len(moving_avg_outliers)} ({len(moving_avg_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# Method 3: STL (Seasonal-Trend decomposition using LOESS)\n",
                "from statsmodels.tsa.seasonal import STL\n",
                "\n",
                "stl = STL(decomp_data['demand'], seasonal=24*7, robust=True)\n",
                "stl_result = stl.fit()\n",
                "\n",
                "stl_residual_threshold = 3 * stl_result.resid.std()\n",
                "stl_outliers_idx = np.abs(stl_result.resid) > stl_residual_threshold\n",
                "stl_outliers = decomp_data[stl_outliers_idx].copy()\n",
                "\n",
                "print(f\"\\nSTL Decomposition (robust):\")\n",
                "print(f\"  Residual std: {stl_result.resid.std():.2f} MW\")\n",
                "print(f\"  Threshold (3σ): {stl_residual_threshold:.2f} MW\")\n",
                "print(f\"  Outliers detected: {stl_outliers_idx.sum()} ({stl_outliers_idx.sum()/len(decomp_data)*100:.2f}%)\")\n",
                "\n",
                "# 10.3 Contextual Anomaly Detection\n",
                "print(\"\\n[10.3] Contextual Anomaly Detection\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Anomalies relative to similar conditions\n",
                "print(f\"\\nContext-based outliers (by hour and day type):\")\n",
                "\n",
                "anomalies_contextual = []\n",
                "\n",
                "for hour in range(24):\n",
                "    for is_weekend in [0, 1]:\n",
                "        mask = (base_df['hour'] == hour) & (base_df['is_weekend'] == is_weekend)\n",
                "        subset = base_df[mask]['demand']\n",
                "        \n",
                "        if len(subset) > 30:  # Enough samples\n",
                "            mean_val = subset.mean()\n",
                "            std_val = subset.std()\n",
                "            \n",
                "            # Find outliers in this context\n",
                "            outlier_mask = mask & (np.abs(base_df['demand'] - mean_val) > 3 * std_val)\n",
                "            context_outliers = base_df[outlier_mask]\n",
                "            \n",
                "            if len(context_outliers) > 0:\n",
                "                anomalies_contextual.extend(context_outliers.index.tolist())\n",
                "\n",
                "anomalies_contextual = list(set(anomalies_contextual))  # Remove duplicates\n",
                "contextual_outliers = base_df.loc[anomalies_contextual].copy()\n",
                "\n",
                "print(f\"  Outliers detected: {len(contextual_outliers)} ({len(contextual_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# Temperature-based contextual anomalies\n",
                "print(f\"\\nTemperature-contextualized outliers:\")\n",
                "\n",
                "temp_bins = pd.cut(base_df['temperature'], bins=10)\n",
                "base_df['temp_bin'] = temp_bins\n",
                "\n",
                "temp_contextual_outliers = []\n",
                "for temp_bin in base_df['temp_bin'].unique():\n",
                "    if pd.notna(temp_bin):\n",
                "        mask = base_df['temp_bin'] == temp_bin\n",
                "        subset = base_df[mask]['demand']\n",
                "        \n",
                "        if len(subset) > 30:\n",
                "            mean_val = subset.mean()\n",
                "            std_val = subset.std()\n",
                "            \n",
                "            outlier_mask = mask & (np.abs(base_df['demand'] - mean_val) > 3 * std_val)\n",
                "            temp_contextual_outliers.extend(base_df[outlier_mask].index.tolist())\n",
                "\n",
                "temp_contextual_outliers = list(set(temp_contextual_outliers))\n",
                "temp_context_outliers = base_df.loc[temp_contextual_outliers].copy()\n",
                "\n",
                "print(f\"  Outliers detected: {len(temp_context_outliers)} ({len(temp_context_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# 10.4 Multivariate Anomaly Detection\n",
                "print(\"\\n[10.4] Multivariate Anomaly Detection\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Local Outlier Factor (LOF)\n",
                "from sklearn.neighbors import LocalOutlierFactor\n",
                "\n",
                "multivar_features = ['demand', 'temperature', 'humidity', 'hour', \n",
                "                     'day_of_week', 'is_weekend']\n",
                "multivar_data = base_df[multivar_features].copy().fillna(base_df[multivar_features].mean())\n",
                "\n",
                "# Normalize features\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "multivar_scaled = scaler.fit_transform(multivar_data)\n",
                "\n",
                "lof = LocalOutlierFactor(n_neighbors=50, contamination=0.01)\n",
                "lof_predictions = lof.fit_predict(multivar_scaled)\n",
                "lof_outliers = base_df[lof_predictions == -1].copy()\n",
                "\n",
                "print(f\"\\nLocal Outlier Factor (LOF):\")\n",
                "print(f\"  Neighbors: 50\")\n",
                "print(f\"  Contamination: 0.01\")\n",
                "print(f\"  Outliers detected: {len(lof_outliers)} ({len(lof_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# One-Class SVM\n",
                "from sklearn.svm import OneClassSVM\n",
                "\n",
                "# Use subset for computational efficiency\n",
                "sample_size = min(10000, len(base_df))\n",
                "sample_indices = np.random.choice(len(base_df), sample_size, replace=False)\n",
                "multivar_sample = multivar_scaled[sample_indices]\n",
                "\n",
                "ocsvm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.01)\n",
                "ocsvm.fit(multivar_sample)\n",
                "ocsvm_predictions = ocsvm.predict(multivar_scaled)\n",
                "ocsvm_outliers = base_df[ocsvm_predictions == -1].copy()\n",
                "\n",
                "print(f\"\\nOne-Class SVM:\")\n",
                "print(f\"  Kernel: RBF\")\n",
                "print(f\"  Nu (contamination): 0.01\")\n",
                "print(f\"  Outliers detected: {len(ocsvm_outliers)} ({len(ocsvm_outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# 10.5 Extreme Event Analysis\n",
                "print(\"\\n[10.5] Extreme Event Analysis\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Top and bottom extreme events\n",
                "n_extreme = 20\n",
                "\n",
                "print(f\"\\nTop {n_extreme} Highest Demand Events:\")\n",
                "top_extreme = base_df.nlargest(n_extreme, 'demand')[\n",
                "    ['time', 'demand', 'temperature', 'hour', 'day_of_week', \n",
                "     'is_weekend', 'is_holiday', 'month']\n",
                "].copy()\n",
                "print(top_extreme.to_string(index=False))\n",
                "\n",
                "print(f\"\\nTop {n_extreme} Lowest Demand Events:\")\n",
                "bottom_extreme = base_df.nsmallest(n_extreme, 'demand')[\n",
                "    ['time', 'demand', 'temperature', 'hour', 'day_of_week', \n",
                "     'is_weekend', 'is_holiday', 'month']\n",
                "].copy()\n",
                "print(bottom_extreme.to_string(index=False))\n",
                "\n",
                "# Sudden demand changes (spikes/drops)\n",
                "base_df['demand_change'] = base_df['demand'].diff()\n",
                "sudden_spikes = base_df.nlargest(n_extreme, 'demand_change')[\n",
                "    ['time', 'demand', 'demand_change', 'temperature', 'hour']\n",
                "].copy()\n",
                "sudden_drops = base_df.nsmallest(n_extreme, 'demand_change')[\n",
                "    ['time', 'demand', 'demand_change', 'temperature', 'hour']\n",
                "].copy()\n",
                "\n",
                "print(f\"\\nTop {n_extreme} Sudden Demand Increases (hour-to-hour):\")\n",
                "print(sudden_spikes.to_string(index=False))\n",
                "\n",
                "print(f\"\\nTop {n_extreme} Sudden Demand Decreases (hour-to-hour):\")\n",
                "print(sudden_drops.to_string(index=False))\n",
                "\n",
                "# 10.6 Anomaly Overlap Analysis\n",
                "print(\"\\n[10.6] Anomaly Detection Method Comparison\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Create summary of all methods\n",
                "all_methods = {\n",
                "    'Z-Score': set(zscore_outliers.index),\n",
                "    'Modified Z-Score': set(modified_zscore_outliers.index),\n",
                "    'IQR': set(iqr_outliers.index),\n",
                "    'Isolation Forest': set(isolation_outliers.index),\n",
                "    'Moving Avg Deviation': set(moving_avg_outliers.index),\n",
                "    'Contextual': set(contextual_outliers.index),\n",
                "    'LOF': set(lof_outliers.index),\n",
                "    'One-Class SVM': set(ocsvm_outliers.index)\n",
                "}\n",
                "\n",
                "print(f\"\\nOutlier counts by method:\")\n",
                "for method, outliers in all_methods.items():\n",
                "    print(f\"  {method:20s}: {len(outliers):5d} ({len(outliers)/len(base_df)*100:.2f}%)\")\n",
                "\n",
                "# Find consensus outliers (detected by multiple methods)\n",
                "outlier_counts = {}\n",
                "for idx in base_df.index:\n",
                "    count = sum(1 for outliers in all_methods.values() if idx in outliers)\n",
                "    if count > 0:\n",
                "        outlier_counts[idx] = count\n",
                "\n",
                "# High-confidence outliers (detected by 4+ methods)\n",
                "consensus_threshold = 4\n",
                "high_confidence_outliers = [idx for idx, count in outlier_counts.items() \n",
                "                            if count >= consensus_threshold]\n",
                "\n",
                "print(f\"\\nConsensus Analysis:\")\n",
                "print(f\"  Detected by 1+ methods: {len(outlier_counts)}\")\n",
                "print(f\"  Detected by 2+ methods: {sum(1 for c in outlier_counts.values() if c >= 2)}\")\n",
                "print(f\"  Detected by 3+ methods: {sum(1 for c in outlier_counts.values() if c >= 3)}\")\n",
                "print(f\"  Detected by 4+ methods: {sum(1 for c in outlier_counts.values() if c >= 4)}\")\n",
                "print(f\"  Detected by 5+ methods: {sum(1 for c in outlier_counts.values() if c >= 5)}\")\n",
                "\n",
                "# Show high-confidence outliers\n",
                "if len(high_confidence_outliers) > 0:\n",
                "    print(f\"\\nHigh-Confidence Outliers (detected by {consensus_threshold}+ methods):\")\n",
                "    high_conf_df = base_df.loc[high_confidence_outliers][\n",
                "        ['time', 'demand', 'temperature', 'hour', 'is_weekend', 'is_holiday']\n",
                "    ].copy()\n",
                "    high_conf_df['detection_count'] = [outlier_counts[idx] for idx in high_confidence_outliers]\n",
                "    high_conf_df = high_conf_df.sort_values('detection_count', ascending=False)\n",
                "    print(high_conf_df.head(20).to_string(index=False))\n",
                "\n",
                "# 10.7 Outlier Characterization\n",
                "print(\"\\n[10.7] Outlier Characterization\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "# Combine all outliers for analysis\n",
                "all_outlier_indices = set()\n",
                "for outliers in all_methods.values():\n",
                "    all_outlier_indices.update(outliers)\n",
                "\n",
                "all_outliers_df = base_df.loc[list(all_outlier_indices)].copy()\n",
                "normal_data = base_df.loc[~base_df.index.isin(all_outlier_indices)].copy()\n",
                "\n",
                "print(f\"\\nOutlier vs Normal Data Comparison:\")\n",
                "print(f\"\\n{'Metric':<25s} {'Normal':>15s} {'Outliers':>15s} {'Difference':>15s}\")\n",
                "print(\"-\"*70)\n",
                "\n",
                "metrics_compare = {\n",
                "    'Mean Demand (MW)': ('demand', 'mean'),\n",
                "    'Std Demand (MW)': ('demand', 'std'),\n",
                "    'Mean Temperature (°C)': ('temperature', 'mean'),\n",
                "    'Weekend Proportion': ('is_weekend', 'mean'),\n",
                "    'Holiday Proportion': ('is_holiday', 'mean'),\n",
                "}\n",
                "\n",
                "for metric_name, (col, func) in metrics_compare.items():\n",
                "    if col in normal_data.columns:\n",
                "        normal_val = getattr(normal_data[col], func)()\n",
                "        outlier_val = getattr(all_outliers_df[col], func)()\n",
                "        diff = outlier_val - normal_val\n",
                "        \n",
                "        print(f\"{metric_name:<25s} {normal_val:>15.2f} {outlier_val:>15.2f} {diff:>+15.2f}\")\n",
                "\n",
                "# Hour distribution\n",
                "print(f\"\\nHour Distribution:\")\n",
                "normal_hours = normal_data['hour'].value_counts(normalize=True).sort_index()\n",
                "outlier_hours = all_outliers_df['hour'].value_counts(normalize=True).sort_index()\n",
                "\n",
                "print(f\"{'Hour':<10s} {'Normal %':>12s} {'Outlier %':>12s}\")\n",
                "for hour in range(24):\n",
                "    normal_pct = normal_hours.get(hour, 0) * 100\n",
                "    outlier_pct = outlier_hours.get(hour, 0) * 100\n",
                "    print(f\"{hour:02d}:00     {normal_pct:>12.2f} {outlier_pct:>12.2f}\")\n",
                "\n",
                "# Visualization: Outlier and Anomaly Detection\n",
                "fig = plt.figure(figsize=(20, 16))\n",
                "gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)\n",
                "\n",
                "# Row 1: Statistical methods\n",
                "# 1.1 Z-Score visualization\n",
                "ax1 = fig.add_subplot(gs[0, 0])\n",
                "sample_size = 5000\n",
                "sample_indices = np.random.choice(len(base_df), sample_size, replace=False)\n",
                "ax1.scatter(range(sample_size), z_scores[sample_indices], alpha=0.5, s=10)\n",
                "ax1.axhline(y=3, color='r', linestyle='--', linewidth=2, label='Threshold (±3)')\n",
                "ax1.axhline(y=-3, color='r', linestyle='--', linewidth=2)\n",
                "ax1.set_xlabel('Sample Index', fontsize=11)\n",
                "ax1.set_ylabel('Z-Score', fontsize=11)\n",
                "ax1.set_title('Z-Score Method', fontsize=12, fontweight='bold')\n",
                "ax1.legend(fontsize=10)\n",
                "ax1.grid(alpha=0.3)\n",
                "\n",
                "# 1.2 IQR Box plot with outliers\n",
                "ax2 = fig.add_subplot(gs[0, 1])\n",
                "bp = ax2.boxplot([base_df['demand'].values], vert=True, patch_artist=True)\n",
                "bp['boxes'][0].set_facecolor('lightblue')\n",
                "ax2.scatter([1]*len(iqr_outliers), iqr_outliers['demand'].values, \n",
                "           color='red', s=20, alpha=0.6, label=f'Outliers (n={len(iqr_outliers)})')\n",
                "ax2.set_ylabel('Demand (MW)', fontsize=11)\n",
                "ax2.set_title('IQR Method', fontsize=12, fontweight='bold')\n",
                "ax2.legend(fontsize=10)\n",
                "ax2.grid(axis='y', alpha=0.3)\n",
                "ax2.set_xticklabels(['Demand'])\n",
                "\n",
                "# 1.3 Isolation Forest scores\n",
                "ax3 = fig.add_subplot(gs[0, 2])\n",
                "iso_scores = iso_forest.score_samples(isolation_data)\n",
                "sample_scores = iso_scores[sample_indices]\n",
                "colors = ['red' if p == -1 else 'blue' for p in isolation_predictions[sample_indices]]\n",
                "ax3.scatter(range(sample_size), sample_scores, c=colors, alpha=0.5, s=10)\n",
                "ax3.set_xlabel('Sample Index', fontsize=11)\n",
                "ax3.set_ylabel('Anomaly Score', fontsize=11)\n",
                "ax3.set_title('Isolation Forest (red=outliers)', fontsize=12, fontweight='bold')\n",
                "ax3.grid(alpha=0.3)\n",
                "\n",
                "# Row 2: Time series methods\n",
                "# 2.1 Decomposition residuals\n",
                "ax4 = fig.add_subplot(gs[1, 0])\n",
                "sample_decomp = decomposition.resid[:24*7]  # 1 week\n",
                "ax4.plot(range(len(sample_decomp)), sample_decomp.values, linewidth=1)\n",
                "ax4.axhline(y=residual_threshold, color='r', linestyle='--', linewidth=2, label='Threshold')\n",
                "ax4.axhline(y=-residual_threshold, color='r', linestyle='--', linewidth=2)\n",
                "ax4.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
                "ax4.set_xlabel('Hour', fontsize=11)\n",
                "ax4.set_ylabel('Residual (MW)', fontsize=11)\n",
                "ax4.set_title('Decomposition Residuals (1 week)', fontsize=12, fontweight='bold')\n",
                "ax4.legend(fontsize=10)\n",
                "ax4.grid(alpha=0.3)\n",
                "\n",
                "# 2.2 Moving average deviation\n",
                "ax5 = fig.add_subplot(gs[1, 1])\n",
                "sample_hours = 7*24\n",
                "sample_data = base_df.iloc[:sample_hours]\n",
                "ax5.plot(sample_data['time'], sample_data['demand'], label='Actual', linewidth=1, alpha=0.7)\n",
                "ax5.plot(sample_data['time'], sample_data['rolling_mean'], \n",
                "        label='Moving Avg (24h)', linewidth=2, color='orange')\n",
                "outlier_mask = sample_data.index.isin(moving_avg_outliers.index)\n",
                "ax5.scatter(sample_data.loc[outlier_mask, 'time'], \n",
                "           sample_data.loc[outlier_mask, 'demand'],\n",
                "           color='red', s=30, label='Outliers', zorder=5)\n",
                "ax5.set_xlabel('Time', fontsize=11)\n",
                "ax5.set_ylabel('Demand (MW)', fontsize=11)\n",
                "ax5.set_title('Moving Average Deviation', fontsize=12, fontweight='bold')\n",
                "ax5.legend(fontsize=9)\n",
                "ax5.grid(alpha=0.3)\n",
                "plt.setp(ax5.xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "# 2.3 STL decomposition\n",
                "ax6 = fig.add_subplot(gs[1, 2])\n",
                "sample_stl = stl_result.resid[:24*7]\n",
                "ax6.plot(range(len(sample_stl)), sample_stl.values, linewidth=1, color='purple')\n",
                "ax6.axhline(y=stl_residual_threshold, color='r', linestyle='--', linewidth=2, label='Threshold')\n",
                "ax6.axhline(y=-stl_residual_threshold, color='r', linestyle='--', linewidth=2)\n",
                "ax6.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
                "ax6.set_xlabel('Hour', fontsize=11)\n",
                "ax6.set_ylabel('STL Residual (MW)', fontsize=11)\n",
                "ax6.set_title('STL Residuals (1 week)', fontsize=12, fontweight='bold')\n",
                "ax6.legend(fontsize=10)\n",
                "ax6.grid(alpha=0.3)\n",
                "\n",
                "# Row 3: Multivariate methods\n",
                "# 3.1 LOF scores\n",
                "ax7 = fig.add_subplot(gs[2, 0])\n",
                "lof_scores = lof.negative_outlier_factor_\n",
                "sample_lof_scores = lof_scores[sample_indices]\n",
                "colors_lof = ['red' if p == -1 else 'blue' for p in lof_predictions[sample_indices]]\n",
                "ax7.scatter(range(sample_size), sample_lof_scores, c=colors_lof, alpha=0.5, s=10)\n",
                "ax7.set_xlabel('Sample Index', fontsize=11)\n",
                "ax7.set_ylabel('LOF Score', fontsize=11)\n",
                "ax7.set_title('Local Outlier Factor (red=outliers)', fontsize=12, fontweight='bold')\n",
                "ax7.grid(alpha=0.3)\n",
                "\n",
                "# 3.2 Method comparison - Venn-style bar chart\n",
                "ax8 = fig.add_subplot(gs[2, 1])\n",
                "method_names = list(all_methods.keys())\n",
                "method_counts = [len(outliers) for outliers in all_methods.values()]\n",
                "colors_methods = plt.cm.Set3(range(len(method_names)))\n",
                "ax8.barh(method_names, method_counts, color=colors_methods, edgecolor='black')\n",
                "ax8.set_xlabel('Number of Outliers', fontsize=11)\n",
                "ax8.set_title('Outliers Detected by Method', fontsize=12, fontweight='bold')\n",
                "ax8.grid(axis='x', alpha=0.3)\n",
                "\n",
                "# 3.3 Consensus outliers\n",
                "ax9 = fig.add_subplot(gs[2, 2])\n",
                "count_distribution = pd.Series(outlier_counts).value_counts().sort_index()\n",
                "ax9.bar(count_distribution.index, count_distribution.values, \n",
                "       color='steelblue', edgecolor='black', alpha=0.7)\n",
                "ax9.set_xlabel('Number of Methods Detecting', fontsize=11)\n",
                "ax9.set_ylabel('Frequency', fontsize=11)\n",
                "ax9.set_title('Consensus Distribution', fontsize=12, fontweight='bold')\n",
                "ax9.grid(axis='y', alpha=0.3)\n",
                "ax9.set_xticks(range(1, 9))\n",
                "\n",
                "# Row 4: Contextual and characterization\n",
                "# 4.1 Outliers in demand time series (full view)\n",
                "ax10 = fig.add_subplot(gs[3, :2])\n",
                "# Downsample for visibility\n",
                "plot_indices = np.arange(0, len(base_df), 24)  # Daily\n",
                "ax10.plot(base_df['time'].iloc[plot_indices], \n",
                "         base_df['demand'].iloc[plot_indices],\n",
                "         linewidth=0.5, alpha=0.7, color='blue', label='Normal')\n",
                "\n",
                "# Plot high-confidence outliers\n",
                "if len(high_confidence_outliers) > 0:\n",
                "    high_conf_times = base_df.loc[high_confidence_outliers, 'time']\n",
                "    high_conf_demands = base_df.loc[high_confidence_outliers, 'demand']\n",
                "    ax10.scatter(high_conf_times, high_conf_demands, \n",
                "                color='red', s=30, alpha=0.8, label=f'High-Confidence Outliers (n={len(high_confidence_outliers)})',\n",
                "                zorder=5)\n",
                "\n",
                "ax10.set_xlabel('Time', fontsize=11)\n",
                "ax10.set_ylabel('Demand (MW)', fontsize=11)\n",
                "ax10.set_title('High-Confidence Outliers in Time Series', fontsize=12, fontweight='bold')\n",
                "ax10.legend(fontsize=10)\n",
                "ax10.grid(alpha=0.3)\n",
                "plt.setp(ax10.xaxis.get_majorticklabels(), rotation=45)\n",
                "\n",
                "# 4.2 Hourly distribution: Normal vs Outliers\n",
                "ax11 = fig.add_subplot(gs[3, 2])\n",
                "x = np.arange(24)\n",
                "width = 0.35\n",
                "normal_hour_counts = normal_data['hour'].value_counts(normalize=True).sort_index() * 100\n",
                "outlier_hour_counts = all_outliers_df['hour'].value_counts(normalize=True).sort_index() * 100\n",
                "\n",
                "# Ensure all hours present\n",
                "for hour in range(24):\n",
                "    if hour not in normal_hour_counts.index:\n",
                "        normal_hour_counts[hour] = 0\n",
                "    if hour not in outlier_hour_counts.index:\n",
                "        outlier_hour_counts[hour] = 0\n",
                "\n",
                "normal_hour_counts = normal_hour_counts.sort_index()\n",
                "outlier_hour_counts = outlier_hour_counts.sort_index()\n",
                "\n",
                "ax11.bar(x - width/2, normal_hour_counts.values, width, label='Normal', alpha=0.7)\n",
                "ax11.bar(x + width/2, outlier_hour_counts.values, width, label='Outliers', alpha=0.7)\n",
                "ax11.set_xlabel('Hour of Day', fontsize=11)\n",
                "ax11.set_ylabel('Percentage (%)', fontsize=11)\n",
                "ax11.set_title('Hourly Distribution: Normal vs Outliers', fontsize=12, fontweight='bold')\n",
                "ax11.set_xticks(range(0, 24, 2))\n",
                "ax11.legend(fontsize=10)\n",
                "ax11.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.savefig(FIGURES_DIR / '07_outlier_anomaly_detection.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\n✓ Figure saved: {FIGURES_DIR / '07_outlier_anomaly_detection.png'}\")\n",
                "\n",
                "# 10.8 Recommendations for Handling Outliers\n",
                "print(\"\\n[10.8] Recommendations for Outlier Handling\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "print(f\"\\nOUTLIER HANDLING STRATEGY:\")\n",
                "print(f\"\\n1. HIGH-CONFIDENCE OUTLIERS (detected by 4+ methods):\")\n",
                "print(f\"   → Count: {len(high_confidence_outliers)}\")\n",
                "print(f\"   → Action: INVESTIGATE thoroughly before modeling\")\n",
                "print(f\"   → Options:\")\n",
                "print(f\"      a) Keep if legitimate extreme events (storms, holidays)\")\n",
                "print(f\"      b) Remove if data errors/measurement issues\")\n",
                "print(f\"      c) Cap/winsorize if using sensitive models\")\n",
                "\n",
                "print(f\"\\n2. MODERATE-CONFIDENCE OUTLIERS (detected by 2-3 methods):\")\n",
                "moderate_outliers = [idx for idx, count in outlier_counts.items() if 2 <= count < 4]\n",
                "print(f\"   → Count: {len(moderate_outliers)}\")\n",
                "print(f\"   → Action: Keep for training (provide learning signal)\")\n",
                "print(f\"   → Monitor: Track model performance on these cases\")\n",
                "\n",
                "print(f\"\\n3. LOW-CONFIDENCE OUTLIERS (detected by 1 method only):\")\n",
                "low_outliers = [idx for idx, count in outlier_counts.items() if count == 1]\n",
                "print(f\"   → Count: {len(low_outliers)}\")\n",
                "print(f\"   → Action: Treat as normal data\")\n",
                "\n",
                "print(f\"\\n4. RECOMMENDED FOR YOUR MODEL:\")\n",
                "print(f\"   ✓ Use robust models: LightGBM, XGBoost handle outliers well\")\n",
                "print(f\"   ✓ Monitor evaluation metrics on outlier periods\")\n",
                "print(f\"   ✓ Consider separate handling for:\")\n",
                "print(f\"      - Extreme weather days\")\n",
                "print(f\"      - National holidays\")\n",
                "print(f\"      - System maintenance periods\")\n",
                "print(f\"   ✓ Feature engineering:\")\n",
                "print(f\"      - Add 'is_extreme_temp' flag\")\n",
                "print(f\"      - Add 'recent_outlier_count' rolling feature\")\n",
                "print(f\"      - Robust scaling instead of standard scaling\")\n",
                "\n",
                "print(f\"\\n5. FOR PRODUCTION MONITORING:\")\n",
                "print(f\"   → Set up real-time anomaly detection\")\n",
                "print(f\"   → Alert thresholds:\")\n",
                "print(f\"      - Demand > {Q3 + 3*IQR:.0f} MW (extreme high)\")\n",
                "print(f\"      - Demand < {Q1 - 3*IQR:.0f} MW (extreme low)\")\n",
                "print(f\"      - Hour-to-hour change > {base_df['demand_change'].quantile(0.99):.0f} MW\")\n",
                "\n",
                "# Save outlier indices for reference\n",
                "outlier_summary = {\n",
                "    'high_confidence_outliers': high_confidence_outliers,\n",
                "    'all_outlier_indices': list(all_outlier_indices),\n",
                "    'method_counts': {method: len(outliers) for method, outliers in all_methods.items()},\n",
                "    'consensus_counts': {\n",
                "        '1+': len(outlier_counts),\n",
                "        '2+': sum(1 for c in outlier_counts.values() if c >= 2),\n",
                "        '3+': sum(1 for c in outlier_counts.values() if c >= 3),\n",
                "        '4+': sum(1 for c in outlier_counts.values() if c >= 4),\n",
                "    }\n",
                "}\n",
                "\n",
                "import json\n",
                "with open(FIGURES_DIR / 'outlier_analysis.json', 'w') as f:\n",
                "    json.dump(outlier_summary, f, indent=2)\n",
                "\n",
                "print(f\"\\n✓ Outlier analysis saved: {FIGURES_DIR / 'outlier_analysis.json'}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "068e1f61",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add to Section 9, after existing key findings:\n",
                "\n",
                "print(\"\\n[6] OUTLIER AND ANOMALY FINDINGS:\")\n",
                "print(f\"  → Total outliers (any method): {len(all_outlier_indices)} ({len(all_outlier_indices)/len(base_df)*100:.2f}%)\")\n",
                "print(f\"  → High-confidence outliers (4+ methods): {len(high_confidence_outliers)} ({len(high_confidence_outliers)/len(base_df)*100:.2f}%)\")\n",
                "print(f\"  → Most effective method: {max(all_methods.items(), key=lambda x: len(x[1]))[0]}\")\n",
                "print(f\"  → Outlier characteristics:\")\n",
                "if len(all_outliers_df) > 0:\n",
                "    print(f\"     Mean demand: {all_outliers_df['demand'].mean():.0f} MW vs {normal_data['demand'].mean():.0f} MW (normal)\")\n",
                "    print(f\"     Most common hour: {all_outliers_df['hour'].mode().values[0] if len(all_outliers_df['hour'].mode()) > 0 else 'N/A'}\")\n",
                "    print(f\"     Weekend proportion: {all_outliers_df['is_weekend'].mean():.1%} vs {normal_data['is_weekend'].mean():.1%} (normal)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f744cacd",
            "metadata": {},
            "outputs": [],
            "source": [
                "## Section: Historical Day Similarity Analysis\n",
                "\n",
                "### Objective\n",
                "\n",
                "# Cell 1: Setup for similarity analysis\n",
                "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
                "from scipy.spatial.distance import cityblock\n",
                "from datetime import timedelta\n",
                "\n",
                "print(\"Setting up historical day similarity analysis...\")\n",
                "\n",
                "# Define features for similarity comparison\n",
                "similarity_features = [\n",
                "    'hour', 'day_of_week', 'month', 'is_weekend', 'is_holiday',\n",
                "    'temperature', 'humidity', 'wind_speed'\n",
                "]\n",
                "\n",
                "# Filter features that exist in the dataframe\n",
                "existing_similarity_features = [f for f in similarity_features if f in df.columns]\n",
                "\n",
                "print(f\"Using {len(existing_similarity_features)} features for similarity:\")\n",
                "print(existing_similarity_features)\n",
                "\n",
                "# Cell 2: Function to find similar historical days\n",
                "def find_similar_days(target_date, df, features, top_n=10, method='euclidean'):\n",
                "    \"\"\"\n",
                "    Find the most similar historical days to a target date\n",
                "    \n",
                "    Args:\n",
                "        target_date: Date to find similar days for\n",
                "        df: DataFrame with historical data\n",
                "        features: List of features to use for similarity\n",
                "        top_n: Number of similar days to return\n",
                "        method: Similarity metric ('euclidean', 'cosine', 'manhattan')\n",
                "    \n",
                "    Returns:\n",
                "        DataFrame with similar days and their similarity scores\n",
                "    \"\"\"\n",
                "    # Get target day data\n",
                "    target_data = df[df['time'].dt.date == target_date]\n",
                "    \n",
                "    if len(target_data) == 0:\n",
                "        print(f\"No data found for {target_date}\")\n",
                "        return None\n",
                "    \n",
                "    # Get historical data (exclude target date and days too close)\n",
                "    min_days_apart = 7  # Minimum days apart to avoid autocorrelation\n",
                "    historical_data = df[\n",
                "        (df['time'].dt.date != target_date) & \n",
                "        (abs((df['time'].dt.date - target_date).dt.days) > min_days_apart)\n",
                "    ].copy()\n",
                "    \n",
                "    # Aggregate daily features (average over the day)\n",
                "    target_features = target_data[features].mean().values.reshape(1, -1)\n",
                "    \n",
                "    # Group historical data by date\n",
                "    historical_daily = historical_data.groupby(historical_data['time'].dt.date)[features].mean()\n",
                "    \n",
                "    # Calculate similarity\n",
                "    if method == 'euclidean':\n",
                "        distances = euclidean_distances(target_features, historical_daily.values)[0]\n",
                "        scores = 1 / (1 + distances)  # Convert to similarity score\n",
                "    elif method == 'cosine':\n",
                "        scores = cosine_similarity(target_features, historical_daily.values)[0]\n",
                "    elif method == 'manhattan':\n",
                "        distances = np.array([cityblock(target_features[0], row) for row in historical_daily.values])\n",
                "        scores = 1 / (1 + distances)\n",
                "    else:\n",
                "        raise ValueError(f\"Unknown method: {method}\")\n",
                "    \n",
                "    # Create results dataframe\n",
                "    similar_days = pd.DataFrame({\n",
                "        'date': historical_daily.index,\n",
                "        'similarity_score': scores\n",
                "    }).sort_values('similarity_score', ascending=False).head(top_n)\n",
                "    \n",
                "    # Add day of week and other info\n",
                "    similar_days['day_of_week'] = similar_days['date'].apply(lambda x: x.strftime('%A'))\n",
                "    similar_days['days_apart'] = similar_days['date'].apply(lambda x: abs((x - target_date).days))\n",
                "    \n",
                "    # Add average demand for these days\n",
                "    similar_days['avg_demand'] = similar_days['date'].apply(\n",
                "        lambda x: df[df['time'].dt.date == x]['demand'].mean()\n",
                "    )\n",
                "    \n",
                "    return similar_days, target_data['demand'].mean()\n",
                "\n",
                "print(\"Function defined successfully\")\n",
                "\n",
                "# Cell 3: Analyze similarity for sample dates\n",
                "# Select sample dates (one from each season if available)\n",
                "sample_dates = []\n",
                "\n",
                "if 'time' in df.columns:\n",
                "    df['month'] = df['time'].dt.month\n",
                "    \n",
                "    # Get one date from each season\n",
                "    winter_dates = df[df['month'].isin([12, 1, 2])]['time'].dt.date.unique()\n",
                "    spring_dates = df[df['month'].isin([3, 4, 5])]['time'].dt.date.unique()\n",
                "    summer_dates = df[df['month'].isin([6, 7, 8])]['time'].dt.date.unique()\n",
                "    fall_dates = df[df['month'].isin([9, 10, 11])]['time'].dt.date.unique()\n",
                "    \n",
                "    if len(winter_dates) > 0:\n",
                "        sample_dates.append(('Winter', winter_dates[len(winter_dates)//2]))\n",
                "    if len(spring_dates) > 0:\n",
                "        sample_dates.append(('Spring', spring_dates[len(spring_dates)//2]))\n",
                "    if len(summer_dates) > 0:\n",
                "        sample_dates.append(('Summer', summer_dates[len(summer_dates)//2]))\n",
                "    if len(fall_dates) > 0:\n",
                "        sample_dates.append(('Fall', fall_dates[len(fall_dates)//2]))\n",
                "\n",
                "print(f\"Selected {len(sample_dates)} sample dates for analysis:\")\n",
                "for season, date in sample_dates:\n",
                "    print(f\"  {season}: {date}\")\n",
                "\n",
                "# Cell 4: Find and visualize similar days\n",
                "results = {}\n",
                "\n",
                "for season, target_date in sample_dates:\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Analyzing {season}: {target_date}\")\n",
                "    print('='*60)\n",
                "    \n",
                "    similar_days, target_demand = find_similar_days(\n",
                "        target_date, df, existing_similarity_features, top_n=10\n",
                "    )\n",
                "    \n",
                "    if similar_days is not None:\n",
                "        print(f\"\\nTarget date average demand: {target_demand:.2f}\")\n",
                "        print(f\"\\nTop 10 most similar historical days:\")\n",
                "        print(similar_days.to_string(index=False))\n",
                "        \n",
                "        results[season] = {\n",
                "            'target_date': target_date,\n",
                "            'target_demand': target_demand,\n",
                "            'similar_days': similar_days\n",
                "        }\n",
                "\n",
                "# Cell 5: Visualize demand patterns for similar days\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, (season, data) in enumerate(results.items()):\n",
                "    if idx < 4:\n",
                "        ax = axes[idx]\n",
                "        target_date = data['target_date']\n",
                "        similar_days = data['similar_days']\n",
                "        \n",
                "        # Get hourly demand for target date\n",
                "        target_hourly = df[df['time'].dt.date == target_date].sort_values('time')\n",
                "        ax.plot(target_hourly['time'].dt.hour, target_hourly['demand'], \n",
                "               linewidth=3, label=f'Target: {target_date}', color='red', marker='o')\n",
                "        \n",
                "        # Plot top 5 similar days\n",
                "        colors = plt.cm.Blues(np.linspace(0.4, 0.9, 5))\n",
                "        for i, (_, row) in enumerate(similar_days.head(5).iterrows()):\n",
                "            similar_date = row['date']\n",
                "            similar_hourly = df[df['time'].dt.date == similar_date].sort_values('time')\n",
                "            \n",
                "            ax.plot(similar_hourly['time'].dt.hour, similar_hourly['demand'],\n",
                "                   alpha=0.6, linewidth=1.5, label=f\"{similar_date} (sim: {row['similarity_score']:.3f})\",\n",
                "                   color=colors[i])\n",
                "        \n",
                "        ax.set_xlabel('Hour of Day')\n",
                "        ax.set_ylabel('Demand')\n",
                "        ax.set_title(f'{season} - Similar Day Patterns')\n",
                "        ax.legend(fontsize=8, loc='best')\n",
                "        ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'similar_days_patterns.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"Similar day patterns visualized\")\n",
                "\n",
                "# Cell 6: Similarity score distribution\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, (season, data) in enumerate(results.items()):\n",
                "    if idx < 4:\n",
                "        ax = axes[idx]\n",
                "        similar_days = data['similar_days']\n",
                "        \n",
                "        # Plot similarity scores\n",
                "        ax.barh(range(len(similar_days)), similar_days['similarity_score'])\n",
                "        ax.set_yticks(range(len(similar_days)))\n",
                "        ax.set_yticklabels([f\"{d} ({dow})\" for d, dow in \n",
                "                            zip(similar_days['date'], similar_days['day_of_week'])],\n",
                "                          fontsize=8)\n",
                "        ax.set_xlabel('Similarity Score')\n",
                "        ax.set_title(f'{season} - Top 10 Similar Days')\n",
                "        ax.grid(True, alpha=0.3, axis='x')\n",
                "        ax.invert_yaxis()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(FIGURES_DIR / 'similarity_scores.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"Similarity scores plotted\")\n",
                "\n",
                "# Cell 7: Demand prediction accuracy using similar days\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"DEMAND PREDICTION ACCURACY USING SIMILAR DAYS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for season, data in results.items():\n",
                "    target_date = data['target_date']\n",
                "    target_demand = data['target_demand']\n",
                "    similar_days = data['similar_days']\n",
                "    \n",
                "    # Calculate prediction using average of top-k similar days\n",
                "    for k in [1, 3, 5]:\n",
                "        predicted_demand = similar_days.head(k)['avg_demand'].mean()\n",
                "        error = abs(predicted_demand - target_demand)\n",
                "        error_pct = (error / target_demand) * 100\n",
                "        \n",
                "        print(f\"\\n{season} ({target_date}):\")\n",
                "        print(f\"  Using top-{k} similar days:\")\n",
                "        print(f\"    Actual demand: {target_demand:.2f}\")\n",
                "        print(f\"    Predicted demand: {predicted_demand:.2f}\")\n",
                "        print(f\"    Error: {error:.2f} ({error_pct:.2f}%)\")\n",
                "\n",
                "# Cell 8: Feature contribution to similarity\n",
                "# Analyze which features contribute most to similarity\n",
                "\n",
                "def analyze_feature_contribution(target_date, df, features):\n",
                "    \"\"\"Analyze which features contribute most to day similarity\"\"\"\n",
                "    \n",
                "    target_data = df[df['time'].dt.date == target_date]\n",
                "    if len(target_data) == 0:\n",
                "        return None\n",
                "    \n",
                "    # Calculate similarity using each feature individually\n",
                "    feature_contributions = {}\n",
                "    \n",
                "    for feature in features:\n",
                "        similar_days, _ = find_similar_days(\n",
                "            target_date, df, [feature], top_n=5\n",
                "        )\n",
                "        \n",
                "        if similar_days is not None:\n",
                "            # Average similarity score using this feature alone\n",
                "            avg_similarity = similar_days['similarity_score'].mean()\n",
                "            feature_contributions[feature] = avg_similarity\n",
                "    \n",
                "    return pd.DataFrame.from_dict(\n",
                "        feature_contributions, \n",
                "        orient='index', \n",
                "        columns=['avg_similarity']\n",
                "    ).sort_values('avg_similarity', ascending=False)\n",
                "\n",
                "# Analyze for one sample date\n",
                "if len(sample_dates) > 0:\n",
                "    sample_season, sample_date = sample_dates[0]\n",
                "    \n",
                "    print(f\"\\nAnalyzing feature contribution for {sample_date}...\")\n",
                "    feature_contrib = analyze_feature_contribution(\n",
                "        sample_date, df, existing_similarity_features\n",
                "    )\n",
                "    \n",
                "    if feature_contrib is not None:\n",
                "        print(\"\\nFeature contribution to similarity:\")\n",
                "        print(feature_contrib)\n",
                "        \n",
                "        # Plot\n",
                "        plt.figure(figsize=(10, 6))\n",
                "        plt.barh(range(len(feature_contrib)), feature_contrib['avg_similarity'])\n",
                "        plt.yticks(range(len(feature_contrib)), feature_contrib.index)\n",
                "        plt.xlabel('Average Similarity Score')\n",
                "        plt.title('Feature Contribution to Day Similarity')\n",
                "        plt.grid(True, alpha=0.3, axis='x')\n",
                "        plt.gca().invert_yaxis()\n",
                "        plt.tight_layout()\n",
                "        plt.savefig(FIGURES_DIR / 'feature_contribution_similarity.png', dpi=300, bbox_inches='tight')\n",
                "        plt.show()\n",
                "\n",
                "# Cell 9: Day-of-week similarity analysis\n",
                "# Check if similar days tend to be the same day of week\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"DAY-OF-WEEK SIMILARITY ANALYSIS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for season, data in results.items():\n",
                "    target_date = data['target_date']\n",
                "    similar_days = data['similar_days']\n",
                "    \n",
                "    target_dow = target_date.strftime('%A')\n",
                "    same_dow_count = (similar_days['day_of_week'] == target_dow).sum()\n",
                "    \n",
                "    print(f\"\\n{season} ({target_date} - {target_dow}):\")\n",
                "    print(f\"  Similar days with same day-of-week: {same_dow_count}/10\")\n",
                "    print(f\"  Day-of-week distribution:\")\n",
                "    print(similar_days['day_of_week'].value_counts().to_string())\n",
                "\n",
                "# Cell 10: Temporal distance vs similarity\n",
                "# Analyze relationship between temporal distance and similarity\n",
                "\n",
                "all_similar_days = []\n",
                "for season, data in results.items():\n",
                "    similar_days = data['similar_days'].copy()\n",
                "    similar_days['season'] = season\n",
                "    all_similar_days.append(similar_days)\n",
                "\n",
                "if len(all_similar_days) > 0:\n",
                "    combined_similar = pd.concat(all_similar_days, ignore_index=True)\n",
                "    \n",
                "    # Plot temporal distance vs similarity\n",
                "    plt.figure(figsize=(12, 6))\n",
                "    \n",
                "    for season in combined_similar['season'].unique():\n",
                "        season_data = combined_similar[combined_similar['season'] == season]\n",
                "        plt.scatter(season_data['days_apart'], season_data['similarity_score'],\n",
                "                   label=season, alpha=0.6, s=100)\n",
                "    \n",
                "    # Add trend line\n",
                "    z = np.polyfit(combined_similar['days_apart'], combined_similar['similarity_score'], 1)\n",
                "    p = np.poly1d(z)\n",
                "    x_trend = np.linspace(combined_similar['days_apart'].min(), \n",
                "                         combined_similar['days_apart'].max(), 100)\n",
                "    plt.plot(x_trend, p(x_trend), \"r--\", linewidth=2, alpha=0.8, label='Trend')\n",
                "    \n",
                "    plt.xlabel('Days Apart from Target')\n",
                "    plt.ylabel('Similarity Score')\n",
                "    plt.title('Temporal Distance vs Similarity Score')\n",
                "    plt.legend()\n",
                "    plt.grid(True, alpha=0.3)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(FIGURES_DIR / 'temporal_distance_vs_similarity.png', dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    # Calculate correlation\n",
                "    correlation = combined_similar['days_apart'].corr(combined_similar['similarity_score'])\n",
                "    print(f\"\\nCorrelation between temporal distance and similarity: {correlation:.3f}\")\n",
                "\n",
                "# Cell 11: Summary statistics\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"SUMMARY: SIMILAR HISTORICAL DAYS ANALYSIS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "summary_stats = {\n",
                "    'avg_similarity_score': combined_similar['similarity_score'].mean(),\n",
                "    'std_similarity_score': combined_similar['similarity_score'].std(),\n",
                "    'avg_days_apart': combined_similar['days_apart'].mean(),\n",
                "    'avg_demand_difference': combined_similar.apply(\n",
                "        lambda row: abs(row['avg_demand'] - results[row['season']]['target_demand']), \n",
                "        axis=1\n",
                "    ).mean()\n",
                "}\n",
                "\n",
                "print(f\"\\nOverall Statistics:\")\n",
                "print(f\"  Average similarity score: {summary_stats['avg_similarity_score']:.3f}\")\n",
                "print(f\"  Std similarity score: {summary_stats['std_similarity_score']:.3f}\")\n",
                "print(f\"  Average days apart: {summary_stats['avg_days_apart']:.1f} days\")\n",
                "print(f\"  Average demand difference: {summary_stats['avg_demand_difference']:.2f}\")\n",
                "\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(\"Key Insights:\")\n",
                "print(f\"{'='*60}\")\n",
                "print(\"1. Similar days can be identified using weather and temporal features\")\n",
                "print(\"2. Demand patterns are highly consistent across similar historical days\")\n",
                "print(\"3. Day-of-week is a strong predictor of similar demand patterns\")\n",
                "print(\"4. Weather conditions play a significant role in similarity\")\n",
                "print(\"5. This validates the use of historical patterns in forecasting\")\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
